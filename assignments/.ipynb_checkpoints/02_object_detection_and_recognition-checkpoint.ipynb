{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2 : Object Detection and Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import tarfile\n",
    "from utils import *\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Template Matching \n",
    "\n",
    "\n",
    "### 2.1.1 Introduction\n",
    "\n",
    "In this warm-up section, we will address the problem of detection and recognition using Template Matching (http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_imgproc/py_template_matching/py_template_matching.html). \n",
    "\n",
    "Template matching is a 'brute-force' algorithm for object recognition. The most basic method of template matching is to directly compare the grayscale images, without using edge detection. For example, if you were trying to detect, let's say a football, you will need to create a base template of the object. During the operation, the template matching algorithm would analyze the current image to find areas which are similar to the template. This basic approach is quite limited. For one thing, it is not robust to inconsistent changes in brightness within the image. If the template image has strong features, a feature-based approach may be considered; the approach may prove further useful if the match in the search image might be transformed in some fashion. For templates without strong features, or for when the bulk of the template image constitutes the matching image, a template-based approach may be effective. \n",
    "\n",
    "In the naive approach, the difference between the template and the matching area is computed pixel by pixel and used to calculate the overall error. It is possible to reduce the number of sampling points by reducing the resolution of the search and template images by the some factor and performing the operation on the resultant downsized images (multiresolution, or Pyramid (image processing)), providing a search window of data points within the search image so that the template does not have to search every viable data point or a combination of both.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"https://pythonspot-9329.kxcdn.com/wp-content/uploads/2015/05/templateMatch.jpeg\"></img>\n",
    "\n",
    "Template matching example. Left: Template image. Right: Input image with the resulting image highlighted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Section objectives: \n",
    "\n",
    "In this section, we will explore the advantages and disadvantages of template matching method. However, in contrast with the previous Chapter, we will leave most of the implementation for the lector (i.e. you).\n",
    "\n",
    "The following section will introduce to the most common metrics used for the matching distance and how to are used in OpenCV. Your task will be to:\n",
    "\n",
    "* Implement each metric *by hand* \n",
    "* compare the accuracy against the OpenCV method \n",
    "* Analyse and report your observations for each metric in 3 exercises and one mini-challenge.\n",
    "\n",
    "As the final exercise, you will be given a set of more \"challenging\" data examples where using what you *learned before*, you will be asked to detect several objects in the scene. Your resulting algorithm __should have the given set of inputs and outputs__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Distance, minimums and maximums\n",
    "\n",
    "The two (and pretty much only) important parts of the Naive Template Matching algorithm is the *distance transform*, i.e. the metric to know if we found a match or not, and the global minima detection. \n",
    "\n",
    "For an  Input image $I$ if size $W\\times H$, a template Image $T$ of size $w\\times h$; ($w<W, h<H$), the distance methods implemented in OpenCv are the following: \n",
    "\n",
    "* Mean Squared Difference Method = CV_TM_SQDIFF\n",
    "\n",
    "\\begin{equation*}\n",
    "R(x,y)= \\sum _{x',y'} (T(x',y')-I(x+x',y+y'))^2 \n",
    "\\end{equation*}\n",
    "\n",
    "* Normalized Mean Squared Difference Method = CV_TM_SQDIFF_NORMED\n",
    "\\begin{equation*}\n",
    "        R(x,y)= \\frac{\\sum_{x',y'} (T(x',y')-I(x+x',y+y'))^2}{\\sqrt{\\sum_{x',y'}T(x',y')^2 \\cdot \\sum_{x',y'} I(x+x',y+y')^2}}\n",
    "\\end{equation*}\n",
    "\n",
    "* Cross Correlation Method = CV_TM_CCORR\n",
    "\\begin{equation*}\n",
    "        R(x,y)= \\sum _{x',y'} (T(x',y') \\cdot I(x+x',y+y'))\n",
    "\\end{equation*}\n",
    "\n",
    "* Normalized Cross Correlation method = CV_TM_CCORR_NORMED\n",
    "\\begin{equation*}\n",
    "        R(x,y)= \\frac{\\sum_{x',y'} (T(x',y') \\cdot I(x+x',y+y'))}{\\sqrt{\\sum_{x',y'}T(x',y')^2 \\cdot \\sum_{x',y'} I(x+x',y+y')^2}}\n",
    "\\end{equation*}\n",
    "\n",
    "* Correlation Coefficient Method = CV_TM_CCOEFF\n",
    "\\begin{equation*}\n",
    "        R(x,y)= \\sum _{x',y'} (T'(x',y') \\cdot I'(x+x',y+y'))\n",
    "\\end{equation*}\n",
    "\n",
    "where\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{array}{l} T'(x',y')=T(x',y') - 1/(w \\cdot h) \\cdot \\sum _{x'',y''} T(x'',y'') \\\\ I'(x+x',y+y')=I(x+x',y+y') - 1/(w \\cdot h) \\cdot \\sum _{x'',y''} I(x+x'',y+y'') \n",
    "\\end{array}\n",
    "\\end{equation*}\n",
    "\n",
    "* Normalized  Normalized Correlation Coefficient = CV_TM_CCOEFF_NORMED\n",
    "\\begin{equation*}\n",
    "        R(x,y)= \\frac{ \\sum_{x',y'} (T'(x',y') \\cdot I'(x+x',y+y')) }{ \\sqrt{\\sum_{x',y'}T'(x',y')^2 \\cdot \\sum_{x',y'} I'(x+x',y+y')^2} }\n",
    "\\end{equation*}\n",
    "\n",
    "\n",
    "Please notice that the dimensions of the output image, $R$, will depend on how you handle the edges. The easiest way is to ser the return an output image of size $(W-w+1, H-h+1)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the function finishes the comparison, the resulting image will contain an image map with the obtained values. In OpenCV, the best matches can be found as global minimums or maximums (depending which matric you used) using the minMaxLoc() function. \n",
    "\n",
    "### Exercise 2.1.4\n",
    "\n",
    "From the functions listed above, indicate if the best matching position is located either in the local minimums or in the maximums. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1.5\n",
    "\n",
    "Implement (at least 3) of the functions listed above, and use them as arguments in the base method provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIXED input images to use\n",
    "img_rgb  = cv.imread('../data/space-invaders_1.jpg')\n",
    "img_gray = cv.imread('../data/space-invaders_1.jpg',0)\n",
    "template = cv.imread('../data/template_0.png',0)\n",
    "\n",
    "def template_matching(input_image, template, distance_function):\n",
    "    \"\"\"\n",
    "    Given an input image, iterates over the image and computes the distance w/r\n",
    "    the template, using a given distance function. \n",
    "\n",
    "    :input_image:       Input image. :) \n",
    "    :template:          The Template Image.\n",
    "    :distance_function  Function used to compute the distance. The function should receive a image patch \n",
    "                        and a template as inputs.\n",
    "    :return:            The distance map.\n",
    "    \"\"\"\n",
    "    W = input_image.shape[1];\n",
    "    H = input_image.shape[0];\n",
    "    \n",
    "    w = template.shape[1];\n",
    "    h = template.shape[0];\n",
    "\n",
    "    # Output image/map\n",
    "    dist_map = np.zeros((H-h+1,W-w+1), dtype=np.float32)\n",
    "\n",
    "    # we could avoid the fors by using lambda functions.\n",
    "    for y in range(dist_map.shape[1]):\n",
    "            for x in range(dist_map.shape[0]):\n",
    "                # We take just the sub-patch where to compute the distance\n",
    "                holder_patch = input_image[x:x+h,y:y+w];\n",
    "                # for each point we compute the distance w/r the template\n",
    "                dist_map[x,y]= distance_function(holder_patch,template); \n",
    "                    \n",
    "    return dist_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distance function example\n",
    "def dummy_distance_fcn(patch, template):\n",
    "    \"\"\"\n",
    "    Compute distance between an image patch and a given template\n",
    "    \n",
    "    :patch: Image patch\n",
    "    :template: Template\n",
    "    :return: Distance\n",
    "    \"\"\"\n",
    "    diff = template.astype(np.float32) - patch.astype(np.float32)\n",
    "    return np.sum(diff ** 2.0)\n",
    "\n",
    "#How to call it:\n",
    "dist_map = template_matching(img_gray, template, dummy_distance_fcn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here another 3 distance functions.\n",
    "\n",
    "# Cross Correlation Method\n",
    "def cross_correlation(patch, template):\n",
    "    \n",
    "    H = patch.shape[0];\n",
    "    W = patch.shape[1];\n",
    "    \n",
    "    h = template.shape[0];\n",
    "    w = template.shape[1];\n",
    "    \n",
    "    distance = np.zeros((H, W))\n",
    "    \n",
    "    for ix in range(0, W):\n",
    "        for iy in range(0, H):\n",
    "            for tx in range(0, w):\n",
    "                for ty in range(0, h):\n",
    "                    distance[ix, iy] += template[tx, ty] * patch[ix + tx, iy + ty]\n",
    "                    \n",
    "    return distance\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shae;  (330, 506)\n",
      "Dist:  (330, 506) (330, 506)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aminmekacher/miniconda3/envs/ntds_project/lib/python3.6/site-packages/ipykernel_launcher.py:21: RuntimeWarning: overflow encountered in ubyte_scalars\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-cabac1145ced>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdist_cross\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_correlation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_gray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#plt.imshow(img_gray)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-4b0009d14c6d>\u001b[0m in \u001b[0;36mcross_correlation\u001b[0;34m(patch, template)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mty\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                     \u001b[0mdistance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miy\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mty\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdistance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dist_cross = cross_correlation(img_gray, template)\n",
    "#plt.imshow(img_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(330, 506)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_gray.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1.6\n",
    "\n",
    "Check your outputs by using the build-in functions in OpenCV to compute the maps for each of the methods implemented. Show in each cell: your map and the OpenCV map.\n",
    "\n",
    "**YOUR ANSWER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example:\n",
    "res = cv.matchTemplate(img_gray,template,cv.TM_SQDIFF)\n",
    "\n",
    "# Display two example maps\n",
    "fig, ax = plt.subplots(1, 2, figsize=(15, 7), squeeze=False)\n",
    "\n",
    "# Show image, add title\n",
    "display_image(dist_map, axes=ax[0][0])\n",
    "ax[0][0].set_title(\"Your distance\")\n",
    "\n",
    "display_image(res, axes=ax[0][1])\n",
    "ax[0][1].set_title(\"OpenCv distance\")\n",
    "\n",
    "#Do the same for your other 3 functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the local minimums/maximums\n",
    "\n",
    "As explained before, in order to find the location of our possible object we need to find the minimum or maximum point in our resulting distance map. OpenCV minMaxLoc() [[doc](https://docs.opencv.org/2.4/modules/core/doc/operations_on_arrays.html#void%20minMaxLoc(InputArray%20src,%20double*%20minVal,%20double*%20maxVal,%20Point*%20minLoc,%20Point*%20maxLoc,%20InputArray%20mask)] can be used to find the local minimum and maximum of the single-channel array (1D or 2D).\n",
    "However, if we happen to have several objects in the image that we would like to match cv.minMaxLoc() won't give you all the locations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1.7\n",
    "\n",
    "Write your own function `multiMinMax(src, flag, params)`, which should take an input 2D image `src` and return an `output_array` with the local minimums or maximums depending on the provided `flag` (`flag = \"min\"` or `flag = \"max\"`), and a given `params`.\n",
    "The `params` can be, for example, a _threshold_ for the local minima/maxima, the maximum number of maximums/minimums to return, a difference between the global maxima/minima to be included, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Multi min-max function\n",
    "def multi_min_max(src, flag, params):\n",
    "    \"\"\"\n",
    "    Find multiple min-max value in a given array.\n",
    "    :src: Array where to look for min/max values\n",
    "    :flag: Type of queries (min/max)\n",
    "    :params: Extrat parameters used by the function\n",
    "    \"\"\"\n",
    "    pos = []\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How should be called:\n",
    "img_rgb  = cv.imread('../data/space-invaders_1.jpg')\n",
    "img_gray = cv.imread('../data/space-invaders_1.jpg',0)\n",
    "template = cv.imread('../data/template_0.png',0)\n",
    "\n",
    "# how to draw the ROIS\n",
    "def draw_rois(src, template, locations, color = (0,0,255)):\n",
    "    w, h = template.shape[::-1]\n",
    "    img_holder = src.copy();\n",
    "    for pt in zip(*locations[::-1]):\n",
    "        cv.rectangle(img_holder, pt, (pt[0] + w, pt[1] + h), color, 2)\n",
    "    return img_holder; \n",
    "\n",
    "# For a given distance Map\n",
    "distance_map_norm_corr = cv.matchTemplate(img_gray,template,cv.TM_CCORR_NORMED)\n",
    "\n",
    "# Use your function here!\n",
    "locations = multi_min_max(distance_map_norm_corr, 'max', 0.95)\n",
    "\n",
    "# Draw the ROIs \n",
    "img_rgb_holder = draw_rois(img_rgb, template, locations)\n",
    "\n",
    "# example:\n",
    "display_image(img_rgb_holder)\n",
    "\n",
    "# How many ROIS did you returned? \n",
    "print(np.array(locations).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1.8\n",
    "\n",
    "Now that you are all set up, use the functions above and *try* to detect ALL the matching objects in the input image (Using template Matching).\n",
    "\n",
    "Rules:\n",
    "\n",
    "* You can use any metric you want, self-implemented or from OpenCV. \n",
    "* You can tweak your multi_min_max to get better results.\n",
    "* For 1) and 2) and 3) you have to do it in grayscale.\n",
    "* 4) can use multi-channel heuristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.8.1 1) Perfect match <3\n",
    "\n",
    "Using the base input provided, plot the input image _showing_ the locations of the matching objects for the 2 provided templates.\n",
    "\n",
    "Follow the code below and provide some insights like:\n",
    "\n",
    "* Why did you choose that given metric?  \n",
    "* How robust to false positives/negatives is your selected metric.\n",
    "* Is the number of output locations the same as the matching objects? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input images to use\n",
    "p1_src_rgb    = cv.imread('../data/space-invaders_1.jpg')\n",
    "p1_src_gray   = cv.imread('../data/space-invaders_1.jpg',0)\n",
    "\n",
    "# Show the matching of these 2 templates:\n",
    "p1_template_1 = cv.imread('../data/template_1.png',0)\n",
    "p1_template_2 = cv.imread('../data/template_2.png',0)\n",
    "\n",
    "display_image(p1_src_gray)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(3, 1),squeeze=False)\n",
    "\n",
    "display_image(p1_template_1, axes=ax[0][0])\n",
    "ax[0][0].set_title(\"Template 1\")\n",
    "ax[0][0].set_xticks([])\n",
    "ax[0][0].set_yticks([])\n",
    "\n",
    "display_image(p1_template_2, axes=ax[0][1])\n",
    "ax[0][1].set_title(\"Template 2\")\n",
    "ax[0][1].set_xticks([])\n",
    "ax[0][1].set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# YOUR CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 2.1.8.2 2) Not so perfect Match </3\n",
    "\n",
    "Plot the input image _showing_ the locations of the matching objects and any false positive. All the _invaders_ in the same row counts as the \"same\" class. \n",
    "\n",
    "Follow the code bellow and provide some insights like:\n",
    "\n",
    "* How did you select the number of maximums/minimus?  \n",
    "* How robust to false positives/negatives is your selected metric.\n",
    "* Is the number of output locations the same as the matching objects? \n",
    "* Could you use any of the features from the last chapter to improve the matching?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input\n",
    "p2_src_rgb    = cv.imread('../data/space-invaders_2.jpg')\n",
    "p2_src_gray   = cv.imread('../data/space-invaders_2.jpg',0)\n",
    "\n",
    "# 2 tempaltes (check that tempalte one don't match all the invaders in the same row) \n",
    "p2_template_1 = cv.imread('../data/template_3.png',0)\n",
    "p2_template_2 = cv.imread('../data/template_1.png',0)\n",
    "\n",
    "display_image(p2_src_gray)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(3, 1),squeeze=False)\n",
    "\n",
    "display_image(p2_template_1, axes=ax[0][0])\n",
    "ax[0][0].set_title(\"Template 1\")\n",
    "ax[0][0].set_xticks([])\n",
    "ax[0][0].set_yticks([])\n",
    "\n",
    "display_image(p2_template_2, axes=ax[0][1])\n",
    "ax[0][1].set_title(\"Template 2\")\n",
    "ax[0][1].set_xticks([])\n",
    "ax[0][1].set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# YOUR CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.8.3 3) Find Waldo.\n",
    "\n",
    "Finding Waldo (or Wally for our Brittish friends) it's a series of children's puzzle books from back in the 80's. Something like finding Nemo, but harder. In this exercise, you will try to find Waldo using template matching. \n",
    "\n",
    "Using the given template (beautifully cut from the input source) use it to find Waldo. \n",
    "Plot the input image _showing_ the location of Waldo (if you find it) and any false positive/negative. \n",
    "\n",
    "In this exercise, you may choose to use any transformation over the input image (like to scale it to save time, or change the color space) or tweak the distance metric. \n",
    "\n",
    "You can't modify the template.\n",
    "\n",
    "Follow the code below and provide some insights like:\n",
    "\n",
    "* What metric seemed to work better this time? \n",
    "* Was it different from the previous exercise?\n",
    "* Could you use any of the features from the last chapter to improve the matching?\n",
    "* How could you reduce the false positives? \n",
    "\n",
    "Save the output in a separate image to visualize it easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "p3_src_rgb  = cv.imread('../data/finding_waldo_1.jpg')\n",
    "p3_src_gray = cv.imread('../data/finding_waldo_1.jpg',0)\n",
    "p3_template = cv.imread('../data/waldo_template.jpg',0)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 3),squeeze=False)\n",
    "\n",
    "display_image(p3_src_rgb, axes=ax[0][0])\n",
    "ax[0][0].set_title(\"Input\")\n",
    "ax[0][0].set_xticks([])\n",
    "ax[0][0].set_yticks([])\n",
    "\n",
    "display_image(p3_template, axes=ax[0][1])\n",
    "ax[0][1].set_title(\"Template\")\n",
    "ax[0][1].set_xticks([])\n",
    "ax[0][1].set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# YOUR CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.8.4 4) Find Waldo 2.0.\n",
    "\n",
    "As you can see, using template matching can be tricky, even when you have a _good_ template. In this last exercise, you will try to design a good __template__ (it can be done in Paint if you want =P) in order to find Waldo in three different images. In this exercise, you can use the information of the 3 RGB channels (and use a _better_ color space for example) or transform the input image (for example to homogenize the scale!).\n",
    "\n",
    "**WARNING:** Pleaes notice that you have tu use the same __template__ to find waldo in the 3 Images!.  \n",
    "\n",
    "Plot your creted template and the input images _showing_ the location of each Waldo (if you find it) and any false positive. \n",
    "\n",
    "Follow the code below and provide some insights like:\n",
    "\n",
    "* Was it different from the previous exercise?\n",
    "* Could you perform better if you could use more than 1 template to find Waldo? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_images = [\"../data/waldo_1.jpg\",\"../data/waldo_2.jpg\",\"../data/waldo_3.jpg\"]\n",
    "loc_images = [\"../data/location_1.png\",\"../data/location_2.png\",\"../data/location_3.png\"]\n",
    "\n",
    "src_color = []\n",
    "src_gray  = []\n",
    "\n",
    "locations = []\n",
    "for img in src_images:\n",
    "    src_color.append(cv.imread(img))\n",
    "    src_gray.append(cv.imread(img,0))\n",
    "    \n",
    "for loc in loc_images:\n",
    "    locations.append(cv.imread(loc))\n",
    "\n",
    "    \n",
    "fig, ax = plt.subplots(3, 2, figsize=(10, 10))\n",
    "\n",
    "for i in range(3):\n",
    "      \n",
    "    # Show image, add title + remove tick\n",
    "    display_image(src_color[i], axes=ax[i][0])\n",
    "    ax[i][0].set_title(\"Waldo %d\"%i)\n",
    "    ax[i][0].set_xticks([])\n",
    "    ax[i][0].set_yticks([])\n",
    "    \n",
    "    display_image(locations[i], axes=ax[i][1])\n",
    "    ax[i][1].set_title(\"Location %d\"%i)\n",
    "    ax[i][1].set_xticks([])\n",
    "    ax[i][1].set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# YOUR CODE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Person Detection \n",
    "\n",
    "In this section, we will return to the HOG features from the last Chapter. As we said before, HOG was proposed as a useful feature for human detection. If you reach this point, you may have noticed that Template matching may not be the best option for this. Imagine how difficult it would be to create a template for any human-shaped structure that you would like to detect as a human in a scene. Instead of that, you will train a Linear Classifier from scratch. \n",
    "\n",
    "Section objectives:\n",
    "\n",
    "In this section, since you know the basics of HOG, you will use OpenCV's implementation to extract the HOG's features of the curated INRIA's Persons dataset to train an SVM Linear classifier (https://en.wikipedia.org/wiki/Support_vector_machine). For this, instead of using OpenCV's (already trained) classifier, we will use the Scikit-learn Machine Learning library (http://scikit-learn.org). Which is one of the most used machine learning libraries around this days.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Dataset \n",
    "\n",
    " This dataset was collected as part of the research work on detection of upright people in images and video. The research is described in detail in the CVPR 2005 paper _Histograms of Oriented Gradients for Human Detection_. The full dataset is about ~1 GB and contains several thousands of pedestrian images. \n",
    " \n",
    "For your convenience, the dataset is already separated into two sets: \n",
    "* \" **_Positives_**\" which are all the images containing at least one person. \n",
    "* \"**_Negatives_**\" any kind of non-human shaped objects images.\n",
    "\n",
    "In addition, the data is already separated in a **training** and **testing** set (seriously, it cannot be more conveniently done).\n",
    "\n",
    "Link:  ftp://ftp.inrialpes.fr/pub/lear/douze/data/INRIAPerson.tar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Data\n",
    "# The following functions will download the data for you and uncompress it\n",
    "\n",
    "url = ' ftp://ftp.inrialpes.fr/pub/lear/douze/data/'\n",
    "data_root = '../data/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "  \"\"\"\n",
    "  Downloads a file if not present, and make sure it's the right size!.\n",
    "  If there's a file with the same name, the function will not try to \n",
    "  download the data set again!\n",
    "  \"\"\"\n",
    "\n",
    "  dest_filename = os.path.join(data_root, filename)\n",
    "    \n",
    "  if force or not os.path.exists(dest_filename):\n",
    "    print('Attempting to download:', filename, 'This may take a while. Please wait.') \n",
    "    filename, _ = urlretrieve(url + filename, dest_filename)\n",
    "    print('\\nDownload Complete!')\n",
    "  statinfo = os.stat(dest_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', dest_filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'The file ' + dest_filename + 'already exist but seems corrupted. Delete it or download it from the browser!')\n",
    "  return dest_filename\n",
    "\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "  \"\"\"\n",
    "  Uncompress the data set for you\n",
    "  \"\"\"\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s dataset (seems to be) already present.\\nSkipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    tar.extractall(data_root)\n",
    "    tar.close()\n",
    "    \n",
    "  print(\"All setup.\")\n",
    "\n",
    "\n",
    "dataset_tar = maybe_download('INRIAPerson.tar', 1016094720)\n",
    "\n",
    "dataset = maybe_extract(dataset_tar)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have problems running the functions above, you can download manually the dataset from the link provided above and uncompress it in your _data_ directory.\n",
    "<br>\n",
    "\n",
    "Once you have the data, you will now process each image on both: _positive_ and _negative_ folders using the OpenCV HOG Descriptor implementation: \n",
    "\n",
    "https://docs.opencv.org/2.4/modules/gpu/doc/object_detection.html\n",
    "\n",
    "\n",
    "Your job: Using the skeleton provided below, for each image in the \"pos\" and \"neg\" folder of the *training* set:\n",
    "\n",
    "* Compute the hog feature vector using the parameters provided below. \n",
    "    * The length of each individual feature vector should be of 16800. Derive bellow why is of this size. (Hint: Imagine that you have an image of size (32,32) which would be the size of this case?).\n",
    "   \n",
    "* Append the feature vector to the _training_feature_ list.\n",
    "    * The total size of this list should be of (16800, number of images in your training set). \n",
    "\n",
    "* For each image add to the _label_ vector a 1 if it's positive or 0 if it's negative. The final length of the _labels_ should be the number of images in your full training set.\n",
    "\n",
    "WARNING: The dataset may contain corrupted images. Be sure, inside your code, to check if the image was loaded properly. Otherwise, you will get either trash features or execution errors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training set:\n",
    "\n",
    "#Positive folder:\n",
    "pos_im_path = \"../data/INRIAPerson/train_64x128_H96/pos/\"\n",
    "#Negative folder:\n",
    "neg_im_path = \"../data/INRIAPerson/train_64x128_H96/neg/\"\n",
    "\n",
    "# Window size: this specifies the size of the input image (remember to scale the input to this size!)\n",
    "winSize     = (64,128)\n",
    "\n",
    "# Size on pixels of each block (remember that a block contains a set of CELLS)\n",
    "blockSize   = (16,16)\n",
    "\n",
    "# The separation between each block. If this value is less than the block size, \n",
    "# there will be overlapping blocks. \n",
    "block_stride = (8,8)\n",
    "\n",
    "# The size of each CELL. Each cell computes one histogram. The cells should FIT inside a block.\n",
    "cellSize = (4,4)\n",
    "\n",
    "# Number of bins for each histogram.\n",
    "nbins = 10\n",
    "\n",
    "# hog is an instance taht contains the info and is able to compute the feature vector.\n",
    "hog = cv2.HOGDescriptor(winSize,blockSize,block_stride,cellSize, nbins)\n",
    "\n",
    "# list to save ALL the features.\n",
    "training_features = []\n",
    "\n",
    "# Auxiliary array to label each features if it comes from a \"positive\"(1) or \"negative\" (0) image.\n",
    "labels = []\n",
    "\n",
    "print (\"Calculating the descriptors for the positive samples and saving them\")\n",
    "\n",
    "# CODE here.{\n",
    "\n",
    "#}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have this features computed and saved in a feature matrix, you are pretty much set to train a classifier. \n",
    "\n",
    "scikit-learn provides all the implementation needed to train a linear classifier with no more than 2 lines of code! (Which is awesome and sad at the same time). \n",
    "\n",
    "For this exercise, you will use a Linear Support Vector Classifier. \n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "\n",
    "Please take the time to read the implementation details, but more importantly, the examples and theory provided in the documentation. If you are not interested in knowing stuff, you can jump right away to use the implemented functions in the block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the classifier.\n",
    "\n",
    "# Check the dimentions fits\n",
    "print (np.array(training_features).shape,len(labels))\n",
    "# (3634, 16800, 1) 3634\n",
    "\n",
    "\n",
    "# create a LINEAR classifier instance here (LinearSVC): \n",
    "\n",
    "\n",
    "#train the classifier (LinearSVC.fit).\n",
    "print (\"Training a Linear SVM Classifier\")\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the classifier -which a fancy way to say that you fitted a 1-D vector of coefficients!- you can use this Support Vector Machine to *classify* if, given an input * HOG feature vector* (with strictly the same dimensions as your training data), it came from an image with a human-shaped form in it (prediction = 1), or not (prediction = 0).\n",
    "\n",
    "*Your job*:\n",
    "\n",
    "For each image in the test folders: \n",
    "* Compute the HOG feature vector.\n",
    "* Predict/classify the vector as positive (1) or negative (0); Hint: LinearSCV.predict(...)\n",
    "* Compute the estimation error for the negative and positive images _separetely_.\n",
    "* Compute and report the F1-score https://en.wikipedia.org/wiki/F1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_neg_path = \"../data/INRIAPerson/test_64x128_H96/neg/\"\n",
    "test_pos_path = \"../data/INRIAPerson/test_64x128_H96/pos/\"\n",
    "\n",
    "print (\"Estimating the test data [Negative samples]\")\n",
    "\n",
    "negative_prediction = []\n",
    "\n",
    "\n",
    "print(\"Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Estimating the test data [Positive samples]\")\n",
    "\n",
    "\n",
    "    \n",
    "print(\"Done\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic classifier above can (should) achieve a success rate of 89% for the positive and less than 2% error for the negative images respectively. \n",
    "\n",
    "Can you tweak the HOG parameters to improve a little bit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try here! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, from the description above, what we created is no more than a *classifier* for only 2 classes (binary classifier): human(human-shaped) or not. In order to create a functional Person detector for arbitrary images or video sequences, some engineering techniques (heuristics) need to be implemented. \n",
    "\n",
    "Pretty much as in template matching, in order to find a person in an arbitrary image you will need to: \n",
    "\n",
    "* Slide your classifier over the full area of the image.\n",
    "* Detect possible matchings. \n",
    "* Report them as positive or negatives\n",
    "* And optionally, repeat the procedure above in different scales, to assure multiscale detection!.\n",
    "\n",
    "The procedure is nicely depicted in the image below for face detection.\n",
    "\n",
    "<img src=\"https://www.pyimagesearch.com/wp-content/uploads/2015/03/sliding-window-animated-adrian.gif\"> \n",
    "</img>\n",
    "_Image taken from: https://www.pyimagesearch.com_\n",
    "\n",
    "For now, we will leave those implementation details for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Face Recognition\n",
    "\n",
    "In this section we will see how it is possible to recognize objects. The study case will target face regognition. Two approaches will be explored, `Eigen Faces` and `Fisher Faces` respectively. These methods are both based dimensional reduction technics listed below:\n",
    "\n",
    "- Principal Component Analysis ([*PCA*](http://www.utdallas.edu/~herve/abdi-awPCA2010.pdf))\n",
    "- Linear Discriminant Analysis (*LDA*)\n",
    "\n",
    "In general, in order to train a recognizer, several steps are needed and can be grouped as follow:\n",
    "\n",
    "- Data preparation\n",
    "- Recognizer training\n",
    "- System validation\n",
    "\n",
    "It is important for the validation step to ensure that the system tested with **unseen** data. By unseen data we mean data that have not been used during the training phase, this will ensure a fair performance assessment without biais. However this does not garantie that the system will *generalize* well to other dataset.\n",
    "\n",
    "### 2.3.1 Data Preparation\n",
    "\n",
    "Data preparation covers various aspect of pre-processing step for training a system. At first, the images need to be splitted into two disctinct subsets thant will be used for `training` and `testing`. In our experiment the dataset used is the *Yale dataset version B*, it includes a total of 38 different identity (*i.e. subject*) each having 20 images undergoing different illumination condition for a total of 760 samples. The splitt will be done by chosing randomly samples from each subjects and placed into the corresponding subset, special care need to be taken in order to avoid having the same example multiple time.\n",
    "\n",
    "The first task is to gather the labels and the images that will be used to train the system. One solution is to store these information into a dictionary where the identity is the key and the pathes to the images for this subject are the values. \n",
    "\n",
    "** Your Answer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    \"\"\"\n",
    "    Scan for images in a given `path` and extract the label as well\n",
    "\n",
    "    :param path:    Path where YaleB dataset is stored\n",
    "    :return:        Dictionary storing the ID and a list of images for this ID\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    # Scan folder\n",
    "    dirs = [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "    # Query images for each subject and extract the subject's ID\n",
    "    \n",
    "    # Your code goes here ...\n",
    "    \n",
    "\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data = load_dataset(os.path.join('..', 'data', 'yaleB'))\n",
    "assert(len(data) == 38)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When all the images and labels have been gathered, the next step is to split into two subsets, the train set and test set. The training set will be composed of $75\\%$ of the images of each subject and the remaining $25\\%$ will be used as test set.\n",
    "\n",
    "Once again the two subsets information will be stored into two separates dictionary similar to what has been done earlier.\n",
    "\n",
    "** Your answer **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data, ratio):\n",
    "    \"\"\"\n",
    "    Split randomly a dataset into two subsets. The ratio provides the distribution for each subset\n",
    "\n",
    "    :param data:    Overall dataset\n",
    "    :param ratio:   Split ratio\n",
    "    :return:        Two dictionaries, train/test\n",
    "    \"\"\"\n",
    "    train = {}\n",
    "    test = {}\n",
    "    \n",
    "    # Your code goes here ...\n",
    "    \n",
    "    \n",
    "    # Iterate over the dataset\n",
    "    \n",
    "\n",
    "    # return subsets    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = split_dataset(data, 0.75)\n",
    "assert(len(train) == 38)\n",
    "assert(len(test) == 38)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with these two subsets, we can load the images and extract features from them. In this case, the pixel intensity will be used as a feature, therefore for an image $I \\in \\mathbb{R}^{ w \\times h}$ the feature vector will have a size of $wh$. This value can be quite large very easily, therefore all images will be downsampled by a factor of $2$.\n",
    "\n",
    "All the training samples will be concatenated into a single matrix where each row is an image (*i.e. flattened*) with dimensions $N \\times K$ where $N$ is the number of samples and $K$ is the dimension of a single image, $K = \\frac{wh}{4}$. The corresponding labels will also be concatenated into a single vector of dimension $N \\times 1$.\n",
    "\n",
    "** Your answer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(subset):\n",
    "    \"\"\"\n",
    "    Load images into one single matrix where each row is one single image (flattened). The final dimensions is [N x K]\n",
    "    where N is the numper of samples available and K is the number of pixel in one image. The original image is first\n",
    "    downspample by a factor of 2\n",
    "\n",
    "    Labels are also exported into a single vector of dimensions [N x 1]\n",
    "\n",
    "    :param subset: Dictionary storing labels/images\n",
    "    :return:       Data matrix and label vector\n",
    "    \"\"\"\n",
    "    \n",
    "    # data = ...\n",
    "    # label = ...\n",
    "    \n",
    "    # Your code goes here ...\n",
    "    \n",
    "    \n",
    "\n",
    "    # Done\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load training images into memery\n",
    "train_img, train_label = load_images(train)\n",
    "# Sanity check\n",
    "assert(train_img.shape[0] == train_label.size)\n",
    "# Output number of samples\n",
    "print(\"There is a total of {} samples for the training set\".format(train_label.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.3.2 Eigenfaces\n",
    "\n",
    "To perform recognition, all pixel's intensities are used as a feature vector. The dimension of these descriptors will be large, therefore a *clever* representation of the data, called subspace, is needed. \n",
    "\n",
    "This subspace is computed using *Principal Component Analysis* method in order to extract meaningfull information and reduce the dimension of the problem. The *PCA* approach is completely unsupervised and extracts the directions, or *basis*, where the variation in the data is the largest inside the feature space. \n",
    "\n",
    "Since we are interested in the variation in the data, the first step is to remove the commmon information present in all samples by subtracting the **average face**. The average is computed using all training samples $I_i$ as follow:\n",
    "\n",
    "$$\n",
    "\\bar{\\boldsymbol{I}} = \\frac{1}{N_t} \\sum_{i=0}^{N_t} \\boldsymbol{I}_i\n",
    "$$\n",
    "\n",
    "where $N_t$ is the total number of training samples and $I_i$ is a specific training sample. Then each samples $I_i$ are normalized as follow:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\phi_i} = \\boldsymbol{I}_i - \\bar{\\boldsymbol{I}}\n",
    "$$\n",
    "\n",
    "With all samples normalized, we need to find a set of orthogonal basis which best explains the distribution of our data. To do so we compute the eigendecomposition of the covariance matrix of the normalized samples.\n",
    "\n",
    "$$\n",
    "\\boldsymbol{C} = \\frac{1}{N_t - 1} \\sum_{i}^{N_t} \\boldsymbol{\\phi}_i\\boldsymbol{\\phi}_i^{\\top} = \\frac{1}{N_t - 1} \\boldsymbol{\\Phi\\Phi}^{\\top}, \\quad \\text{where } \\Phi = \\left[\\boldsymbol{\\phi}_0, \\dots, \\boldsymbol{\\phi}_{N} \\right]\n",
    "$$\n",
    "\n",
    "Find the eigenvectors $u_k$ and the eigenvalues $\\lambda_k$. \n",
    "\n",
    "So far the dimensions of the problem have not been reduced. Moreover the size of the covariance matrix will be $K \\times K$ with $K = \\frac{wh}{4}$. Therefore we will find $K$ eigenvectors representing the variation in our data. To reduce the dimension we will select only the eigenvectors that contribute the most to the variation and dropping the one with little influence. Doing so will reduce the dimension of the subspace to $K \\times K_m$.\n",
    "\n",
    "The question is how to properly determine this $K_m$ value. It can be done by using the eigenvalues computed earlier. These values are representing the energy each vector contribute to. Therefore it is possible to determine the number of bases to select in order to retain a certain amount of energy.\n",
    "\n",
    "$$\n",
    "\\frac{\\sum_{k=0}^{K_m}\\lambda_k}{\\sum_{i=0}^{K}\\lambda_i} < \\Theta\n",
    "$$\n",
    "\n",
    "Where $\\Theta$ represents the amount of energy to retain, which usually is around $95\\%$ but can vary depending on the application. \n",
    "Finally the subspace is defined as:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{U} = \\left[\\boldsymbol{u}_0, \\dots, \\boldsymbol{u}_{K_m}\\right], \\quad \\boldsymbol{U} \\in \\mathbb{R}^{K \\times K_m}\n",
    "$$\n",
    "\n",
    "In practice, the PCA decomposition is computed using `sklearn.decomposition.PCA`, more information can be found [here](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html).\n",
    "\n",
    "For now, compute the face subspace on the training data by retaining $95\\%$ of the variance present in the training data.\n",
    "Once the subspace is computed, display the first $8$ modes or *eigenfaces* and comment on what you see, what do you think are the limitations of such approach?\n",
    "\n",
    "** Your answer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute EigenFaces with PCA while keeping 95% of the variance\n",
    "retain_var = 0.95\n",
    "# Compute Eigenfaces\n",
    "pca = # Code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first four eigenfaces\n",
    "fig, ax = plt.subplots(2, 4, figsize=(12, 7))\n",
    "for k in range(8):\n",
    "    # Define eigenface\n",
    "    \n",
    "    # Your code goes here ...\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our fresh subspace computed we can try to visualize if it separates properly the different subjects. Having a proper plot of an $N$ dimensional space is not feasible, however, we can use only a few components (*i.e. 2 or 3*) of our projected samples to visualize them. \n",
    "\n",
    "\n",
    "What we want is a subspace that is able to separates and clusters properly each subject in order to avoid miss recognition. The code snippet below shows the $3^{rd}$ and $4^{th}$ components on a $2D$ plane.\n",
    "\n",
    "What do you think about it, do we have clean inter-subject separation?\n",
    "\n",
    "** Your answer **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Poject data onto subspace\n",
    "proj_train = pca.transform(train_img)\n",
    "\n",
    "# Visualize \n",
    "# Colors for distinct individuals\n",
    "colors = LabelEncoder().fit_transform(train_label.ravel())\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.scatter(proj_train[:, 2], proj_train[:, 3], c=colors)\n",
    "plt.xlabel('PC2')\n",
    "plt.ylabel('PC3')\n",
    "plt.title('Trainset clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our learned subspace we need to defined the corresponding representation for the training samples. This can be done by projecting them into the eigen subspace (*i.e. eigenface*) as follow:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\omega}_i = \\boldsymbol{U}^\\top \\boldsymbol{\\phi}_i\n",
    "$$\n",
    "\n",
    "In the training set, there are multiple samples avaible for each subject. Their projection won't be exactly the same, therefore we need to have a more generic representation of each person. To do so, we average all representation of the specific person to have his general representation.\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\Omega}_k = \\frac{1}{N_k} \\sum_{p} \\boldsymbol{\\omega}_p\n",
    "$$\n",
    "\n",
    "where $N_k$ is the number of samples for subject $k$, $\\boldsymbol{\\omega}_p$ represents the projected samples of subject $k$ respectively. In our case, $k$ goes from $0$ to $37$.\n",
    "\n",
    "Now implement the function below that computes each subject's centroid, and returned their corresponding labels as well.\n",
    "\n",
    "** Your answer **\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_centroids(data, trsfrm, label):\n",
    "    \"\"\"\n",
    "    Given a list of training samples, compute the centroids of each uniques labels\n",
    "\n",
    "    :param data:    Matrix with all feature vectors stored as row\n",
    "    :param trsfrm:  Embeddings to use (object with `transform(X)` method available such as PCA/LDA from sklearn)\n",
    "    :param label:   List of corresponding labels\n",
    "    :return:        Centroids, unique labels\n",
    "    \"\"\"\n",
    "\n",
    "    # extract unique label\n",
    "    unique_lbl = np.unique(label)\n",
    "\n",
    "    # Your code goes here ...\n",
    "    \n",
    "\n",
    "    # Done\n",
    "    return centroids, unique_lbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define centroids\n",
    "train_centroids, train_centroid_label = compute_centroids(train_img, pca, train_label)\n",
    "assert(train_centroids.shape[0] == 38)\n",
    "assert(train_centroid_label.shape[0] == 38)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've learned a face representation and computed the descriptor for the training samples. Now we can use them to recognize a face. To do so the first step is to bring the *new* sample into our face *subspace* by projection, similarly  to what has been done before:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\omega}_n = \\boldsymbol{U}^{\\top}(\\boldsymbol{I}_n - \\bar{\\boldsymbol{I}})\n",
    "$$\n",
    "\n",
    "where $U$ is the face subspace, $\\bar{\\boldsymbol{I}}$ is the average face learned from the training data and $\\boldsymbol{I}_n$ is the new sample to recognize.\n",
    "\n",
    "Once the sample is in the same subspace as the training samples, we can measure its **similarity** (*distance*) with the centroids $\\boldsymbol{\\Omega}_k$ computed before. The predicted label will be the one corresponding the closest centroid. \n",
    "\n",
    "$$ \n",
    "min \\left|\\left| \\boldsymbol{\\omega}_n - \\boldsymbol{\\Omega}_k \\right|\\right| \\quad \\forall k \\in \\{Train\\}\n",
    "$$\n",
    "\n",
    "Using the prototype below implement a function that predicts each label of new samples.\n",
    "\n",
    "** Your answer **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize(trsfrm, centroids, centroids_label, samples):\n",
    "    \"\"\"\n",
    "    Perform object recognition on a given list of ``amples\n",
    "\n",
    "    :param trsfrm:          Embeddings to use (object with `transform(X)` method available such as PCA/LDA from sklearn)\n",
    "    :param centroids:       List of centroids learned in training phase\n",
    "    :param centroids_label: Label corresponding to the centroids\n",
    "    :param samples:         List of samples to recognize\n",
    "    :return:                Predicted labels\n",
    "    \"\"\"\n",
    "\n",
    "    # Define output container\n",
    "    pred = np.zeros((samples.shape[0], 1), dtype=np.float32)\n",
    "\n",
    "    # Your code goes here ...\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Done\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to validate the implementation is to try the whole system on the training set. The expected recognition accuracy should by close to 100% depending on the task difficulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recognize training set\n",
    "pca_train_pred = recognize(pca, train_centroids, train_centroid_label, train_img)\n",
    "\n",
    "# Compute performance\n",
    "n_err = np.count_nonzero(np.where(pca_train_pred != train_label))\n",
    "train_acc = 1.0 - n_err / train_label.size\n",
    "print(\"The recognition accuracy on the trainig set is {:.2f}\".format(train_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate the recognition accuracy on the testing set and comments on what you see\n",
    "\n",
    "** Your answer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring accuracy of the system is a good indicator of the overall performance but does not indicate where the system is performing poorly. This can be quantified using **Confusion Matrix**. It describes the performance of classification model and shows how the system is confused for each sample in the training set.\n",
    "\n",
    "Such representation can be computed using `sklearn.metrics.confusion_matrix`, details are provided [here](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html).\n",
    "\n",
    "** Your answer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here ...\n",
    "cm = #Code here\n",
    "classes = # Code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(12, 7))\n",
    "plot_confusion_matrix(cm, classes, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Fisherface\n",
    "\n",
    "The subspace computed before with *PCA* was looking at directions where the variation in the data is maximum without paying attention to the class each data point belongs to. Therefore this approach is unsupervised. The major drawback is that the class separability is not guaranteed to be optimum. \n",
    "\n",
    "The approach of *Linear Discriminant Analysis* is to find a subspace where the variation is large (*i.e. similar to PCA*) but also to maximize the inter-class separability by taking into account each sample's label. The figure below shows an example:\n",
    "\n",
    "<img src=\"../data/lda_example.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Such subspace can be computed as follow:\n",
    "\n",
    "- Compute the scatter matrices (*intra-classes* / *inter-classes*)\n",
    "- Compute the eigenvectors / eigenvalues\n",
    "- Select the $K_m$ largest eigenvalues and their corresponding eigenvectors\n",
    "\n",
    "Given a set of samples $\\boldsymbol{I}_0, \\dots, \\boldsymbol{I}_N$ and their corresponding labels $y_0, \\dots, y_N$, the intra-class scatter matrix is computed as follow:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{S}_w = \\sum_{i=1}^N \\left(\\boldsymbol{I}_i - \\boldsymbol{\\mu}_{y_i}\\right) \\left(\\boldsymbol{I}_i - \\boldsymbol{\\mu}_{y_i}\\right)^{\\top}\n",
    "$$\n",
    "\n",
    "where $\\boldsymbol{\\mu}_{k}$ is the sample mean of the $k^{th}$ class.\n",
    "Then the inter-class scatter matrix is defined as:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{S}_b = \\sum_{k=1}^{m} n_k (\\boldsymbol{\\mu}_k - \\boldsymbol{\\mu})(\\boldsymbol{\\mu}_k - \\boldsymbol{\\mu})^{\\top}\n",
    "$$\n",
    "\n",
    "where $m$ is the number of classes, $\\boldsymbol{\\mu}$ is the overall sample average and $n_k$ is the number of samples in the $k^{th}$ class.\n",
    "\n",
    "Finally the subspace $\\boldsymbol{W}$ can be computed by solving the following generalizeed eigenvalue problem:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{S}_b \\boldsymbol{W} = \\lambda \\boldsymbol{S}_w \\boldsymbol{w}\n",
    "$$\n",
    "\n",
    "Finally at most $m-1$ generalized eigenvectors are useful to discriminate between $m$ classes.\n",
    "\n",
    "In practice, such decomposition can be computed using `sklearn.discriminant_analysis.LinearDiscriminantAnalysis`, more information available [here](http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html).\n",
    "\n",
    "Now compute the *LDA* subspace similar to what you have done before and display the first 8 bases (*i.e. Fisherface*).\n",
    "\n",
    "**Your answer **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute FisherFaces with LDA\n",
    "n_component = #code here \n",
    "# Compute Eigenfaces\n",
    "lda = #  Code here \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basis\n",
    "# Code here\n",
    "\n",
    "\n",
    "# Display first four eigenfaces\n",
    "fig, ax = plt.subplots(2, 4, figsize=(12, 7))\n",
    "\n",
    "for k in range(8):\n",
    "    fisherface = #Define here.\n",
    "    \n",
    "    # Show image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to what we have done before visualize the subspace created by the *LDA* decomposition. \n",
    "\n",
    "What do you see, is it better than before ?\n",
    "\n",
    "** Your answer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Poject data onto subspace\n",
    "proj_train = lda.transform(train_img)\n",
    "\n",
    "# Visualize \n",
    "plt.figure(figsize=(12, 7))\n",
    "plt.scatter(proj_train[:, 2], proj_train[:, 3], c=colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on what has been done before, compute the recognition accuracy on the *training*/testing* set for the *LDA* recognizer.\n",
    "\n",
    "** Your answer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here ...\n",
    "\n",
    "print(\"The recognition accuracy on the trainig set is {:.2f}\".format(train_acc))\n",
    "\n",
    "\n",
    "print(\"The recognition accuracy on the testing set is {:.2f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the *Confusion Matrix* and comment on the result you have\n",
    "\n",
    "** Your answer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code goes here ...\n",
    "cm = #Define here\n",
    "classes = #Code here; \n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(12, 7))\n",
    "plot_confusion_matrix(cm, classes, normalize=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have implemented / tested two approaches for face recognition which one works the best and why ? What's are the pro/cons of each method ?\n",
    "\n",
    "** Your answer **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

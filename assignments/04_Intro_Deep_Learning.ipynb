{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Deep Learning\n",
    "\n",
    "\n",
    "Part 1  - Data preprocessing\n",
    "------------\n",
    "\n",
    "\n",
    "The objective of this first part is to learn about simple data curation practices. Data curation (for machine learning) consists basically in analyse, label, and separate in classes your input data.  In section two, you used pre-curated and separated data from the INRIA's person data set. Your first task is to create your training/testing sets by hand and analyse how well \"balance\" they are. \n",
    "\n",
    "**Objectives** \n",
    "In the following sections, you will use this part to \"feed\" both, a classic _Swallow_ (not-Deep) classificator using handcrafted features and a Deep (kind of Deep) neural network. Your task will consist in analyse the accuracy of the aforementioned classification based on the amount of data available; from few hundred of samples to the full data-set.\n",
    "\n",
    "In this section we will provide the general steps and, as in the previous section, you will be asked to search in the function parameters and syntaxis in users documentation.\n",
    "\n",
    "\n",
    "## Dataset \n",
    "\n",
    "In this last section of the course we will use the the [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) dataset. This dataset is designed to look like the classic [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, but, more close to _real data_ scenarios — it's not trivial, and the data is a lot less 'clean' than MNIST.\n",
    "\n",
    "\n",
    "### Libraries: \n",
    "\n",
    "Be sure that you can import all the libraries below; for the next part you will make use of the tensorflow library to implement the used neural network. Be sure to be able to import it as: \n",
    "\n",
    "`` import tensorflow as tf``\n",
    "\n",
    "following the documentation page \n",
    "\n",
    "https://www.tensorflow.org/install/\n",
    "\n",
    "We will use only CPU's based training. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import tarfile\n",
    "from utils import *\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import LinearSVC\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the data. \n",
    "\n",
    "As in section 2, you need to download the data and set the input directory. Be sure to have at about ~2 Gb of free space. If the function is not able to download the data, try on the MNIST site. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Data\n",
    "# The following functions will download the data for you and uncompress it\n",
    "\n",
    "# WARNING:  These varaibles set the input/output paths for ALL the bellow functions.\n",
    "url = 'http://commondatastorage.googleapis.com/books1000/'\n",
    "last_percent_reported = None\n",
    "data_root = os.path.join('..', 'data')\n",
    "\n",
    "\n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "  \"\"\"\n",
    "  Downloads a file if not present, and make sure it's the right size!.\n",
    "  If there's a file with the same name, the function will not try to \n",
    "  download the dataset again!\n",
    "  \"\"\"\n",
    "\n",
    "  dest_filename = os.path.join(data_root, filename)\n",
    "    \n",
    "  if force or not os.path.exists(dest_filename):\n",
    "    print('Attempting to download:', filename, 'This may take a while. Please wait.') \n",
    "    filename, _ = urlretrieve(url + filename, dest_filename)\n",
    "    print('\\nDownload Complete!')\n",
    "  statinfo = os.stat(dest_filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified', dest_filename)\n",
    "  else:\n",
    "    raise Exception(\n",
    "      'The file ' + dest_filename + 'already exist but seems corrupted. Delete it or download it from the browser!')\n",
    "  return dest_filename\n",
    "\n",
    "\n",
    "def try_to_extract(filename, force=False):\n",
    "  \"\"\"\n",
    "  Uncompress the data set for you\n",
    "  \"\"\"\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s dataset (seems to be) already present.\\nSkipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    tar.extractall(data_root)\n",
    "    tar.close()\n",
    "    \n",
    "  data_folders = [os.path.join(root, d) for d in sorted(os.listdir(root)) if os.path.isdir(os.path.join(root, d))]\n",
    "  print(\"All setup.\")\n",
    "  return data_folders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloads if needed.\n",
    "large_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "small_filename  = maybe_download('notMNIST_small.tar.gz', 8458043)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts if able.\n",
    "large_folders = try_to_extract(large_filename)\n",
    "small_folders = try_to_extract(small_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_folders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "When working with data, always check your data. \n",
    "\n",
    "Create a description of your input data. Describe in a table or list (one for each sample size): \n",
    "\n",
    "* Number of classes (characters)\n",
    "* Number of samples per class\n",
    "* General information on the image size and number of channels.\n",
    "\n",
    "Visualize one sample per class bellow for a chosen size (large or small). \n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that you have all your images set up we will load the data into a more manageable format. Since depending on your computer setup you might not be able to fit it all in memory, in here you will use a very useful dataformar in order to separate each class into a separate dataset, store them on disk and curate them independently. \n",
    "\n",
    "To do this we will use pickles.\n",
    "\n",
    "https://docs.python.org/3.2/library/pickle.html\n",
    "\n",
    "“Pickling” is the process whereby a Python object (it can be anything!) is converted into a byte stream (binary format), and “unpickling” is the inverse operation. We will use pickles to save the FULL set of images for each character in one pickle. The result will be a 3D array (image index, x, y) of floating point values, normalized to have approximately zero mean and standard deviation ~0.5 to make the training easier down the road. This process is known as \"normalizing the data\" or \"feature scaling\", which is very important to ensure convergence in the optimization step, as well to ensure that the feature space is well defined.\n",
    "\n",
    "https://en.wikipedia.org/wiki/Feature_scaling\n",
    "\n",
    "\n",
    "Your task, using the skeleton function bellow: \n",
    "\n",
    "* 1) Load all the images in FLOAT format for each class (A,..,J), 1 channel only.\n",
    "* 2) Transform each image intensities such that the range goes from -125,125 (instead of 0, 256)\n",
    "* 3) Scale the function so the new range goes from -0.5 to 0.5.\n",
    "\n",
    "A few images might not be readable, we'll just skip them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" Image fixed size  \"\"\"\n",
    "image_size  = 28     # Pixel width and height. (28x28)\n",
    "pixel_depth = 255.0  # Number of levels per pixel. (0,255)\n",
    "\n",
    "\"\"\" There's should be enough data at the end\"\"\"\n",
    "min_num_images_train = 45000;\n",
    "min_num_images_test  = 1800;\n",
    "\n",
    "\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "    \n",
    "    \"\"\" Base function: \n",
    "\n",
    "      Complete this function to read a each iamge of a given character (folder)\n",
    "      Transforms and scale the image to have 0 mean and standard deviation of ~0.5.\n",
    "\n",
    "      Params: \n",
    "          folder: input character folder (e.g. ../data/notMNIST_large/A/)\n",
    "          min_num_images: minimum number of images you should have per character.\n",
    "\n",
    "      returns: \n",
    "          dataset: Vector containing the fully loaded and scaled dataset.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    image_files = os.listdir(folder)\n",
    "    \n",
    "    # Array size (should be preserved)  \n",
    "    dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "    print(folder)\n",
    "    num_images = 0\n",
    "\n",
    "    # List of all the images inside the folder  \n",
    "    for image in image_files:\n",
    "        image_file = os.path.join(folder, image)\n",
    "    \n",
    "        # If the image is not loadable (there are some corrupted images you can skip them) \n",
    "        try:\n",
    "          ##--- CODE HERE:\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "          ##--- End of your code.\n",
    "\n",
    "          # here I check that you load them correctly and save it in the dataset array.  \n",
    "            if image_data.shape != (image_size, image_size):\n",
    "                raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "            dataset[num_images, :, :] = image_data\n",
    "\n",
    "            num_images = num_images + 1\n",
    "        \n",
    "        #except IOError as e:\n",
    "            #print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "        except:\n",
    "            print('Could not read:', image_file, '- it\\'s ok, skipping.')\n",
    "    \n",
    "    dataset = dataset[0:num_images, :, :]\n",
    "\n",
    "    # If this theshhold is not met, you are doind something wrong (probably)  \n",
    "    if num_images < min_num_images:\n",
    "        raise Exception('Many fewer images than expected: %d < %d' %\n",
    "                    (num_images, min_num_images))\n",
    " \n",
    "    #Check this output! \n",
    "    # The mean should be very close to 0 i.e < 1 and the std should be less than 0.5.  \n",
    "    print('Full dataset tensor:', dataset.shape)\n",
    "    \n",
    "    # Notice we aree calling this a \"tensor\"\n",
    "    print('Mean:', np.mean(dataset))\n",
    "    print('Standard deviation:', np.std(dataset))\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look!, Morty, I'm a Pickle!        \n",
    "\n",
    "# This function calls your pre-defined-function load_letter(folder, min_num_images) and creates the pickle!\n",
    "\n",
    "def Im_a_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "    \n",
    "    \"\"\" Base function: \n",
    "  \n",
    "      Loads all the images listed in data_folders and creates a .pickle file\n",
    "      \n",
    "      Params: \n",
    "          data_folders: list of the folders to pickle (i.e. large_folders, small_folders)\n",
    "          min_num_images: minimum number of images you should have per character.\n",
    "      \n",
    "      returns: \n",
    "          dataset_names: Vector containing all the pickles names.\n",
    "  \"\"\"\n",
    "    dataset_names = []\n",
    "\n",
    "    for folder in data_folders:\n",
    "        set_filename = folder + '.pickle'\n",
    "        dataset_names.append(set_filename)\n",
    "    \n",
    "        if os.path.exists(set_filename) and not force:\n",
    "          # You may override by setting force=True.\n",
    "          print('%s already present - Skipping pickling.' % set_filename)\n",
    "        else:\n",
    "          print('Turning myself into a Pickle! %s.' % set_filename)\n",
    "\n",
    "          dataset = load_letter(folder, min_num_images_per_class)\n",
    "\n",
    "          try:\n",
    "            with open(set_filename, 'wb') as f:\n",
    "              pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "          except Exception as e:\n",
    "            print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "    return dataset_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If everything was done correctly we can then call the following functions without error!\n",
    "\n",
    "Notice that we are here considering the \"large\" data set as our training data set and the small as our test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_datasets = Im_a_pickle(large_folders, 45000)\n",
    "test_datasets  = Im_a_pickle(small_folders, 1800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Problem 2\n",
    "\n",
    "---------\n",
    "\n",
    "To corroborate that our data is properly saved and scaled, display one example per class letter (A,...,J), from the train dataset **or** the test dataset. Include information about the new dynamic range of values of the images.\n",
    "\n",
    "To do this, you will need to use the ``pickle.load(...)``. Check the documentation above for more details. You can use the inhered matplotlib function to show each example. Include a colorbar showing the __values range of the image__.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Finally if everything is correct, the above function should contain the full length of each character sample.  The labels will be stored into a separate array of *integers 0 through 9*.\n",
    "\n",
    "Corroborate that the train_sets are in the order of ~52,000 images, and the train_set in the order of 1,870 images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_sets_sizes(data_set):\n",
    "    \n",
    "    number_files = [];\n",
    "\n",
    "    \"\"\" Base function: \n",
    "  \n",
    "      Loads all the images listed in data_set and return it' s size\n",
    "    \"\"\"\n",
    "        \n",
    "    #Code here \n",
    "\n",
    "    \n",
    "    return number_files\n",
    "\n",
    "print('The number of data of each class of train datasets is, ',data_sets_sizes(train_datasets))\n",
    "print('The number of data of each class of test datasets is, ',data_sets_sizes(test_datasets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 \n",
    "### Creating sub-sampled datasets.\n",
    "\n",
    "\n",
    "In order to evaluate the performance of our classifiers, we need to create subsets of our data properly randomized; this means that we shouldn't choose always the first set of images to compare since we will introduce a bias because of the sampled order. A very nice post on this topic can be found below in case you wonder if it's worth the trouble.\n",
    "\n",
    "https://machinelearningmastery.com/randomness-in-machine-learning/\n",
    "\n",
    "\n",
    "Here, you have to write a function: ``sample_training_data(...)`` which should create a training dataset of a given size, containing aprox. the same number of samples for each label (-1 or +1 samples) _randomly selected_ from the ``train-dataset``; as well as the labels of the training set coded as integers from 0 (A) to 9 (J). \n",
    "\n",
    "Is worth mentioning that is common practice in machine learning to set aside a third dataset known as the _validation dataset_. So, in addition, you will create this extra dataset which is used to prevent overfitting and other training problems. This dataset should __NOT__ contain any image used in the traning dataset. A nice and short explanation on why this is used is presetned below (it also contains nice code hints relevant to the exercise ;) )\n",
    "\n",
    "https://machinelearningmastery.com/difference-test-validation-datasets/\n",
    "\n",
    "\n",
    "In summary:\n",
    "\n",
    "* Create a function that returns a subset of you data. \n",
    "* The function needs to ensure a balanced subset selection. \n",
    "* The function should also return (if needed) a validation dataset.\n",
    "* The returned validation dataset must not contain any repeated sample from the training dataset. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "#Code here.\n",
    "def sample_training_data(pickle_files, train_size, validation_size=0):\n",
    "\n",
    "    \"\"\" Base function: \n",
    "  \n",
    "      Given a train size and a validation size returns a ndarray containing a total of \n",
    "      <train_size/number_of_clases> samples, and <validation_size/number_of_clases> \n",
    "      samples for each class.\n",
    "      \n",
    "      Example: For a train_size = 100, validation_size = 0, the returned n_array should contain 100/10 = 10 \n",
    "      samples of each character and no validation dataset. \n",
    "      \n",
    "      The samples should be chose randomly.\n",
    "      \n",
    "      Params: \n",
    "          pickle_files: list of the pickle files (training set)\n",
    "          train_size: total length of the new training set\n",
    "          validation_size: total length of the validation set\n",
    "      \n",
    "      returns: \n",
    "          train_dataset: ndarray containing all the training images (properly normalized)\n",
    "          train_labels : the labels of each selected image. \n",
    "          validation_dataset: ndarray containing all the validatiion images (properly normalized)\n",
    "          validation_labels: the labels of each selected image. \n",
    "  \"\"\"\n",
    "    # Initialize the output dataset\n",
    "    image_size = 28\n",
    "    train_dataset = np.zeros((train_size,image_size,image_size))\n",
    "    train_labels = np.zeros(train_size)\n",
    "    \n",
    "    validation_dataset = []\n",
    "    validation_labels  = []\n",
    "    \n",
    "    if(validation_size>0):\n",
    "        validation_dataset = np.zeros((validation_size,image_size,image_size))\n",
    "        validation_labels = np.zeros(validation_size)\n",
    "        \n",
    "        \n",
    "    #CODE HERE\n",
    "\n",
    "    \n",
    "    return train_dataset, train_labels, validation_dataset, validation_labels\n",
    "  \n",
    "\n",
    "# EXAMPLE OF USE\n",
    "train_size = 20000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "train_dataset, train_labels, validation_dataset, validation_labels = sample_training_data(train_datasets, train_size,valid_size);\n",
    "\n",
    "test_dataset, test_labels, _, _ = sample_training_data(test_datasets, test_size);\n",
    "\n",
    "print('Training size: ', train_dataset.shape, '\\nLabel vector size:',train_labels.shape)\n",
    "print('Testing size: ', test_dataset.shape, '\\nLabel vector size:',test_labels.shape)\n",
    "print('Validation size: ', validation_dataset.shape, '\\nLabel vector size:',test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you need to randomize the vector so it doesn't follow any specific order: like first all the A characters and then the 'B' characters, and so on \n",
    "\n",
    "```(A, A, ..., A , B, B, ..., B, C, C, ...,C,... )```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to randomize THE ORDER of a given dataset.\n",
    "# Be sure that the dataset and the labels are shuffled in the same order so they MATCH.\n",
    "\n",
    "def randomize(dataset, labels):\n",
    "    \n",
    "    #code here\n",
    "\n",
    "    \n",
    "    \n",
    "    return shuffled_dataset, shuffled_labels\n",
    "\n",
    "\n",
    "# Shuffle the three datasets  sets\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels   = randomize(test_dataset, test_labels)\n",
    "validation_dataset, validation_labels  = randomize(test_dataset, test_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Show us that your method works and all the datasets are coherent with the labels. You can display the shuffled order and show the first images for the three datasets. They should match the labels.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 5\n",
    "---------\n",
    "\n",
    "By default, this dataset might contain a lot of overlapping samples (repeated images with different names). As explaining before, this overlap between training, test and validation, can skew the results and cause overfitting. \n",
    "\n",
    "To ensure this doesn't happend, measure how much overlap there is between training, validation and test samples. \n",
    "\n",
    "- Check for overlapping samples in your dataset using any matric that you would like. Take into account that, if you use a computational expensive metric, this process will take very long times. Consider using a _clever_ and fast metric.\n",
    "\n",
    "- Modify your ``sample_training_data``function and provide a curated train, validation and test dataset removing repeated samples from one of them.  Write the new function bellow.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Problem 6\n",
    "---------\n",
    "\n",
    "Let's get an idea of what a basic classifier can give you on this data. \n",
    "\n",
    "Train a simple model on this data using 50, 100, 1000 and 5000 training samples. \n",
    "\n",
    "Hint: you can use the ```LogisticRegression``` or ```LogisticRegressionCv``` model from sklearn.linear_model.\n",
    "\n",
    "Provide a score for the prediction over the full test data set. You can use any metric from the previous chapters or an implemented one like the ```cross_val_score``` form sklearn which is more accurate.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html\n",
    "\n",
    "A good choice of parameters (and regularization method) can give you results up to the 89%\n",
    "\n",
    "```\n",
    "#Samples: 50 ---> Score: 0.471428571429\n",
    "#Samples: 100 ---> Score: 0.605865717935\n",
    "#Samples: 1000 ---> Score: 0.760772183027\n",
    "#Samples: 5000 ---> Score: 0.812826972435 \n",
    "```\n",
    "\n",
    "\n",
    "When you optimize the parameters, be sure to do it only to improve the resultss on the test dataset, and never use the validation dataset (otherwise it loses it purpuse).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def train_and_validate(num_examples):\n",
    "    \n",
    "    # code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sizes = [50, 100, 1000, 5000]\n",
    "\n",
    "print(\"Cross Validation Score\\n\")\n",
    "for size in training_sizes:\n",
    "    score = train_and_validate(size)\n",
    "    print(\"Samples:\", size,\"---> Score:\", score)\n",
    "    \n",
    "    \n",
    "#Samples: 50 ---> Score: 0.471428571429\n",
    "#Samples: 100 ---> Score: 0.605865717935\n",
    "#Samples: 1000 ---> Score: 0.760772183027\n",
    "#Samples: 5000 ---> Score: 0.812826972435\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2) Deep Learning - Intro to Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Linear Model and Multi-layer Perceptron (MLP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the last couple of years, deep learning has produced state-of-the-art results in many computer vision tasks. In this section, a practical overview of deep Learning's basic concepts has been provided and we are going to work with a simple linear and multi-layer perceptron (MLP) models.\n",
    "\n",
    "At its core, deep learning is a class of neural network models. That is a model with an input layer, an output layer, and an arbitrary number of hidden layers. These layers are made up of neurons or neural units. They are called neurons because they share some similarities with the behaviour of the neurons present in the human brain. For our purposes, we can think of a neuron as a nonlinear function of the weighted sum of its inputs. Since the neuron is really the most basic part of any deep learning model it is a good place to start. \n",
    "\n",
    "As the starting point, we continue to work the notMNIST dataset. After loading the dataset, we define and optimize a simple mathematical model in TensorFlow. The results are then plotted and discussed. First, we import tensorflow with other needed packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using your function defined above create a training, test, and validation dataset. The dataset should be splitted into 3 mutually exclusive sub-sets (200000 training images, 10000 validation and 10000 test images, respectively). You can display the images in each sub-set. They should match the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here:\n",
    "\n",
    "\n",
    "# Display the openend files\n",
    "print(\"Training Set \", train_dataset.shape, train_labels.shape)\n",
    "print(\"Validation Set\", validation_dataset.shape, validation_labels.shape)\n",
    "print(\"Test Set\", test_dataset.shape,test_dataset.shape)\n",
    "\n",
    "\n",
    "#Training Set  (20000, 28, 28) (20000,)\n",
    "#Validation Set (10000, 28, 28) (10000,)\n",
    "#Test Set (10000, 28, 28) (10000, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 28\n",
    "IMAGE_SHAPE=(IMAGE_SIZE,IMAGE_SIZE)\n",
    "\n",
    "def plot_images(images, cls_true, cls_pred=None):\n",
    "    assert len(images) == len(cls_true) == 9\n",
    "    \n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(IMAGE_SHAPE), cmap='binary')\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the first images from the test-set.\n",
    "images = test_dataset[0:9]\n",
    "\n",
    "# Get the true classes for those images.\n",
    "cls_true = test_labels[0:9]\n",
    "\n",
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(images=images, cls_true=cls_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Hot Encoding and Image Flattened Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our labels contain a list of predictions for our examples, e.g. [1, 9, ...]. In tensorflow and many other machine learning API's, the label has to be converted to the encoding format as so-called One-Hot encoding. This means the labels have been converted from a single number to a vector whose length equals the number of possible classes. All elements of the vector are zero except for the $i$'th element which is one and means the class is $i$.\n",
    "\n",
    "In addition, because we are going to use linear layers and multiplications of the neural network, you always want your data to be a (1 or) 2-dimensional matrix, where each row is the vector representing your data. Therefore, it would be more complicated and less efficient without reshaping images first. Here, we need to flatten images before passing them to our model. As an example, please print out the One-Hot encoded labels for the first 5 images in the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reformat(dataset, labels):\n",
    "    \"\"\"\n",
    "        Reformat the data to the one-hot and flattened mode\n",
    "    \"\"\"\n",
    "    n_dataset = dataset.reshape((-1, IMAGE_SIZE * IMAGE_SIZE)).astype(np.float32)\n",
    "\n",
    "    # Convert to the one hot format\n",
    "    n_labels = (np.arange(NUM_LABELS) == labels[:, None]).astype(np.float32)\n",
    "\n",
    "    return n_dataset, n_labels\n",
    "\n",
    "\n",
    "NUM_LABELS = 10\n",
    "\n",
    "TRAIN_DATASET, TRAIN_LABELS = reformat(train_dataset, train_labels)\n",
    "VALID_DATASET, VALID_LABELS = reformat(validation_dataset, validation_labels)\n",
    "TEST_DATASET, TEST_LABELS = reformat(test_dataset, test_labels)\n",
    "\n",
    "# Display the files\n",
    "print(\"Training Set \", TRAIN_DATASET.shape, TRAIN_LABELS.shape)\n",
    "print(\"Validation Set\", VALID_DATASET.shape, VALID_LABELS.shape)\n",
    "print(\"Test Set\", TEST_DATASET.shape, TEST_LABELS.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_LABELS[0:5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Graph\n",
    "\n",
    "The entire purpose of TensorFlow is to have a so-called computational graph that can be executed much more efficiently than if the same calculations were to be performed directly in Python. TensorFlow can be more efficient than NumPy because TensorFlow knows the entire computation graph that must be executed, while NumPy only knows the computation of a single mathematical operation at a time.\n",
    "\n",
    "TensorFlow can also automatically calculate the gradients that are needed to optimize the variables of the graph to make the model perform better. This is because the graph is a combination of simple mathematical expressions so the gradient of the entire graph can be calculated using the chain rule for derivatives.\n",
    "\n",
    "\n",
    "A TensorFlow graph consists of the following parts which will be detailed below:\n",
    "\n",
    "* Placeholder variables used to change the input to the graph.\n",
    "* Model variables that are going to be optimised to make the model perform better.\n",
    "* The model which is essentially just a mathematical function that calculates some output given the input in the placeholder variables and the model variables.\n",
    "* A cost measure that can be used to guide the optimization of the variables.\n",
    "* An optimization method which updates the variables of the model.\n",
    "\n",
    "In addition, the TensorFlow graph may also contain various debugging statements, e.g. for logging data to be displayed using TensorBoard, which is not covered here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default graph\n",
    "\n",
    "As the starting point, we create a new computational graph via the `tf.Graph` constructor. To add operations to this graph, we must register it as the default graph. The way the TensorFlow API is designed, library routines that create new operation nodes always attach these to the current default graph. We register our graph as the default by using it as a Python context manager in a `with-as` statement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placeholder variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholder variables serve as the input to the graph that we may change each time we execute the graph. We call this feeding the placeholder variables and it is demonstrated further below.\n",
    "\n",
    "First, we define the placeholder variable for the input images including train and validation set. This allows us to change the images that are input to the TensorFlow graph. This is a so-called tensor, which just means that it is a multi-dimensional vector or matrix. The data-type is set to `float32` and the shape is set to `[None, IMAGE_SIZE_FLAT]`, where `None` means that the tensor may hold an arbitrary number of images with each image being reshaped into a vector of length `IMAGE_SIZE_FLAT` which is equal to IMAGE_SIZE * IMAGE_SIZE. In addition, we create a constant tensor for the validation and test set since they are fixed and will be evaluated later.   \n",
    "\n",
    "\n",
    "Next we have the placeholder variable for the true labels associated with the images that were input in the placeholder variable TF_TRAIN_DATASET. The shape of this placeholder variable is [None, NUM_LABELS] which means it may hold an arbitrary number of labels and each label is a vector of length NUM_LABELS which is 10 in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables to be optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the placeholder variables that were defined above and which serve as feeding input data into the model, there are also some model variables that must be changed by TensorFlow to make the model perform better on the training data.\n",
    "\n",
    "In fact, the model variables are the network hidden layer parameters including layer's weights and biases. The first variable that must be optimized are WEIGHTS and defined here as a TensorFlow variable that must be initialized and whose shape is [IMAGE_SIZE_FLAT, NUM_LABELS] for the simple linear model, so it is a 2-dimensional tensor (or matrix) with IMAGE_SIZE_FLAT rows and NUM_LABELS columns. There are various ways to initialize the weights like initialization with zeros tf.zeros, tf.truncated_normal and tf.random_normal, which output random values from a normal distribution and you can specify the mean and The standard deviation (stddev) of the normal distribution.\n",
    "\n",
    "The second variable that must be optimized is called BIASES and is defined as a 1-dimensional tensor (or vector) of length NUM_LABELS.\n",
    "\n",
    "Last but not least, you should name every important operation in your code. In complex models, it is good practice to use scopes. The important point is that if you want to later use some operation, you have to either name it or put it into a collection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "This simple mathematical model multiplies the training images in the placeholder variable TF_TRAIN_DATASET with the WEIGHTS and then adds the BIASES.\n",
    "\n",
    "The result is a matrix of shape [NUM_IMAGES, NUM_LABELS] because TF_TRAIN_DATASET has shape [NUM_IMAGES, IMAGE_SIZE_FLAT] and WEIGHTS has shape [IMAGE_SIZE_FLAT, NUM_LABELS], so the multiplication of those two matrices is a matrix with shape [NUM_IMAGES, NUM_LABELS] and then the BIASES vector is added to each row of that matrix.\n",
    "\n",
    "However, these estimates are a bit rough and difficult to interpret because the numbers may be very small or large, so we want to normalize them so that each row of the LOGITS matrices sums to one (see below code), and each element is limited between zero and one. This is calculated using the so-called softmax function tf.nn.softmax.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost-function to be optimized\n",
    "\n",
    "To make the model better at classifying the input images, we must somehow change the variables for `WEIGHTS` and `BIASES`. To do this, we first need to know how well the model currently performs by comparing the predicted output of the model to the desired output.\n",
    "\n",
    "The cross-entropy is a performance measure used in classification. The cross-entropy is a continuous function that is always positive and if the predicted output of the model exactly matches the desired output then the cross-entropy equals zero. The goal of optimization is, therefore, to minimise the cross-entropy, so it gets as close to zero as possible by changing the `WEIGHTS` and `BIASES` of the model.\n",
    "\n",
    "TensorFlow has a built-in function for calculating the cross-entropy using `tf.nn.softmax_cross_entropy_with_logits`. Note that it uses the values of the `LOGITS` in train, validation and test sets because it also calculates the softmax internally. In order to use the cross-entropy to guide the optimization of the model's variables we need a single scalar value, so we simply take the average of the cross-entropy using (`tf.reduce_mean`) for all the image classifications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization method\n",
    "\n",
    "Now that we have a cost measure that must be minimized, we can then create an optimizer. In this case it is the basic form of Gradient Descent where the step-size is set to 0.5.\n",
    "\n",
    "Note that optimization is not performed at this point. In fact, nothing is calculated at all, we just add the optimizer-object to the TensorFlow graph for later execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE_FLAT=IMAGE_SIZE * IMAGE_SIZE\n",
    "\n",
    "# Create a new graph\n",
    "GRAPH = tf.Graph()\n",
    "\n",
    "# Register the graph as the default one to add nodes\n",
    "with GRAPH.as_default():\n",
    "    \n",
    "    # Define placeholders\n",
    "    TF_TRAIN_DATASET = tf.placeholder(tf.float32, shape=(None, IMAGE_SIZE * IMAGE_SIZE))\n",
    "    TF_TRAIN_LABELS = tf.placeholder(tf.float32, shape=(None, NUM_LABELS))\n",
    "    TF_VALID_DATASET = tf.constant(VALID_DATASET)\n",
    "    TF_TEST_DATASET = tf.constant(TEST_DATASET)\n",
    "    \n",
    "    with tf.name_scope(\"Linear_model\"):\n",
    "        \n",
    "        \"\"\"\n",
    "           Initialize weights and biases\n",
    "        \"\"\"\n",
    "        \n",
    "        WEIGHTS = tf.Variable(tf.random_normal(shape=[IMAGE_SIZE_FLAT, NUM_LABELS], stddev=0.1))\n",
    "        BIASES = tf.Variable(tf.zeros([NUM_LABELS]))\n",
    "    \n",
    "        \"\"\"\n",
    "           Compute the logits WX + b\n",
    "        \"\"\" \n",
    "        TRAIN_LOGITS = tf.matmul(TF_TRAIN_DATASET, WEIGHTS) + BIASES\n",
    "        VALID_LOGTIS = tf.matmul(TF_VALID_DATASET, WEIGHTS) + BIASES\n",
    "        TEST_LOGITS = tf.matmul(TF_TEST_DATASET, WEIGHTS) + BIASES\n",
    "    \n",
    "        \"\"\"\n",
    "           Softmax function\n",
    "        \"\"\"   \n",
    "        TRAIN_PREDICTION = tf.nn.softmax(TRAIN_LOGITS)\n",
    "        VALID_PREDICTION = tf.nn.softmax(VALID_LOGTIS)\n",
    "        TEST_PREDICTION = tf.nn.softmax(TEST_LOGITS)\n",
    "    \n",
    "        \"\"\"\n",
    "           Cost-function\n",
    "        \"\"\"\n",
    "        CROSS_ENTROPY= tf.nn.softmax_cross_entropy_with_logits_v2(logits=TRAIN_LOGITS, labels=TF_TRAIN_LABELS)\n",
    "        COST= tf.reduce_mean(CROSS_ENTROPY)\n",
    "        # Optimizer\n",
    "        OPTIMIZER = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(COST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    \"\"\"\n",
    "        Divides the number of true predictions to the number of total predictions\n",
    "    \"\"\"\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1)) / predictions.shape[0])\n",
    "\n",
    "def print_confusion_matrix(predictions, labels):\n",
    "\n",
    "    # Convert the One-Hot encoded vectors to a single number by taking the index of the highest element\n",
    "    labels=np.array([label.argmax() for label in labels])\n",
    "    predictions = np.array([prediction.argmax() for prediction in predictions])\n",
    "\n",
    "    # Get the confusion matrix using sklearn.\n",
    "    cm = confusion_matrix(y_true=labels,\n",
    "                          y_pred=predictions)\n",
    "\n",
    "    # Print the confusion matrix as text.\n",
    "    print(cm)\n",
    "\n",
    "    # Plot the confusion matrix as an image.\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "\n",
    "    # Make various adjustments to the plot.\n",
    "    plt.tight_layout()\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(NUM_LABELS)\n",
    "    plt.xticks(tick_marks, range(NUM_LABELS))\n",
    "    plt.yticks(tick_marks, range(NUM_LABELS))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "\n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Run\n",
    "\n",
    "### Create TensorFlow session\n",
    "\n",
    "Once the TensorFlow graph has been created, we have to create a TensorFlow session which is used to execute the graph and train our model. For this, we enter a session environment using a `tf.Session` as a context manager. We pass our graph  object to its constructor, so that it knows which graph to manage. To then execute nodes, we have several options. The   most general way is to call Session.run() and pass a list of tensors we wish to compute. Alternatively, we may call `eval()` on  tensors and `run()` on  operations directly.\n",
    "\n",
    "Before evaluating any other node, we must first ensure that the variables in our graph are initialized. Theoretically,  we could `run` the `Variable.initializer` operation for each variable. However, one most often just uses the\n",
    "`tf.initialize_all_variables()` utility operation provided by TensorFlow, which in turn executes the `initializer` \n",
    "operation for each `Variable` in the graph. Then, we can perform a certain number of iterations of stochastic gradient  descent, fetching an example and label from the notMNIST dataset each time and feeding it to the run routine. \n",
    "\n",
    "One important point is that, there are 200,000 images in the training-set. It takes a long time to calculate the gradient of the model using all these images. We therefore use Stochastic Gradient Descent which only uses a small batch of images in each iteration of the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "NUM_ITERATIONS = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "with tf.Session(graph=GRAPH) as session:\n",
    "    \"\"\"\n",
    "        Start the above variable initialization\n",
    "    \"\"\"\n",
    "    init = tf.global_variables_initializer()\n",
    "    session.run(init)\n",
    "\n",
    "    print(\"Variables initialized\")\n",
    "\n",
    "    for step in range(NUM_ITERATIONS):\n",
    "        \"\"\"\n",
    "            Generate a random base and then generate a minibatch\n",
    "        \"\"\"\n",
    "        BASE = (step * BATCH_SIZE) % (TRAIN_LABELS.shape[0] - BATCH_SIZE)\n",
    "        BATCH_DATA = TRAIN_DATASET[BASE:(BASE + BATCH_SIZE), :]\n",
    "        BATCH_LABELS = TRAIN_LABELS[BASE:(BASE + BATCH_SIZE), :]\n",
    "        \"\"\"\n",
    "            Feed the current session with batch data\n",
    "        \"\"\"\n",
    "        FEED_DICT = {TF_TRAIN_DATASET: BATCH_DATA, TF_TRAIN_LABELS: BATCH_LABELS}\n",
    "        _, l, predictions = session.run([OPTIMIZER, COST, TRAIN_PREDICTION], feed_dict=FEED_DICT)\n",
    "\n",
    "        if(step % 500 == 0):\n",
    "            print(\"Minibatch loss at step \", step, \": \", l)\n",
    "            print(\"Minibatch accuracy: \", accuracy(predictions, BATCH_LABELS))\n",
    "            print(\"Validation accuracy: \", accuracy(VALID_PREDICTION.eval(), VALID_LABELS))\n",
    "            \n",
    "        if (step == NUM_ITERATIONS-1):\n",
    "            print(\"Test accuracy: \", accuracy(TEST_PREDICTION.eval(), TEST_LABELS))\n",
    "            print_confusion_matrix(TEST_PREDICTION.eval(), TEST_LABELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 7: Comment on the accuracy of the classifier, what is the meaning of each entry, and comment the confussion matrix results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron (MLP)\n",
    "\n",
    "The next architecture we are going to work with is multilayer perceptron (MLP). An MLP can be viewed as a logistic regression classifier where the input is first transformed using some non-linear transformations of the intermediate network layers. These intermediate layers are referred to as a `hidden layers`. Here, we are going to define TF graph for the MLP model with two hidden layers and one output layer. First, let’s draw the model the MLP represents:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import Image \n",
    "Image(filename='data/multilayer-perceptron-drawing.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 8\n",
    "\n",
    "In our example above, the MLP model has two hidden layers with sizes of `HIDDEN_NODES_1=102` and `HIDDEN_NODES_2=51`, respectively. In a MLP, every element of a previous layer is connected to every element of the next layer. For example, the weights in the second hidden layer has `shape=[HIDDEN_NODES_1, HIDDEN_NODES_2]`. \n",
    "\n",
    "Now, lets define MLP graph with two hidden layers (in the following exercise you should only use train and test sets):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Define the number of nodes for the hidden layers\n",
    "HIDDEN_NODES_1=102\n",
    "HIDDEN_NODES_2=51\n",
    "\n",
    "MLP_GRAPH = tf.Graph()\n",
    "\n",
    "#Code Here.\n",
    "with MLP_GRAPH.as_default():\n",
    "    \n",
    "    # Define placeholders\n",
    "    TF_TRAIN_DATASET = tf.placeholder(tf.float32, shape=(None, IMAGE_SIZE * IMAGE_SIZE))\n",
    "    TF_TRAIN_LABELS = tf.placeholder(tf.float32, shape=(None, NUM_LABELS))\n",
    "    TF_TEST_DATASET = tf.constant(TEST_DATASET)\n",
    "    \n",
    "    with tf.name_scope(\"Linear_model\"):\n",
    "        \n",
    "        #Code Here.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 9\n",
    "\n",
    "As mentioneded before, in an MLP, the number of parameters (weights w_i and b_i)to learn is proporcional to the number of layers and neurons on each of those layers. In here make the computation of the number of parameters for the network that you constructed above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-function to perform optimization iterations\n",
    "\n",
    "Here, we build a function for performing a number of optimization iterations so as to gradually improve the weights and biases of the MLP model. In each iteration, a new batch of data is selected from the training-set and then TensorFlow executes the optimizer using those training samples. The inputs of the function are the number of iterations `NUM_ITERATIONS` and number of used samples `NUM_SAMPLES` to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(NUM_ITERATIONS, NUM_SAMPLES):\n",
    "    \n",
    "    with tf.Session(graph=MLP_GRAPH) as session:\n",
    "\n",
    "        \"\"\"\n",
    "            Start the above variable initialization\n",
    "        \"\"\"\n",
    "        # tf.initialize_all_variables().run()  \n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        print(\"Variables initialized\")\n",
    "\n",
    "        for step in range(NUM_ITERATIONS):\n",
    "            \"\"\"\n",
    "                Select the desired samples\n",
    "            \"\"\"\n",
    "            TRAIN_DATASET_S = TRAIN_DATASET[:NUM_SAMPLES]\n",
    "            TRAIN_LABELS_S = TRAIN_LABELS[:NUM_SAMPLES]\n",
    "            \"\"\"\n",
    "                Generate a random base and then generate a minibatch\n",
    "            \"\"\"\n",
    "            \n",
    "            indices = np.random.choice(range(TRAIN_LABELS_S.shape[0]\n",
    "                                             ), BATCH_SIZE)\n",
    "            BATCH_DATA = TRAIN_DATASET_S[indices, :]\n",
    "            BATCH_LABELS = TRAIN_LABELS_S[indices, :]\n",
    "\n",
    "            \"\"\"\n",
    "                Feed the current session with batch data\n",
    "            \"\"\"\n",
    "            FEED_DICT = {TF_TRAIN_DATASET: BATCH_DATA, TF_TRAIN_LABELS: BATCH_LABELS}\n",
    "            _, l, predictions = session.run([OPTIMIZER, COST, TRAIN_PREDICTION], feed_dict=FEED_DICT)\n",
    "\n",
    "            if (step == NUM_ITERATIONS - 1):\n",
    "                acc=accuracy(TEST_PREDICTION.eval(), TEST_LABELS)\n",
    "                print(\"Test accuracy: \", accuracy(TEST_PREDICTION.eval(), TEST_LABELS))\n",
    "\n",
    "    return acc\n",
    "\n",
    "optimize(NUM_ITERATIONS=100, NUM_SAMPLES=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras API\n",
    "\n",
    "Keras is a high-level API for tensorflow used to build deep learning models in a fast and more \"user friendly\" way. The API was “designed for human beings, not machines,” and “follows best practices for reducing cognitive load.” Neural layers, cost functions, optimizers, initialization schemes, activation functions, and regularization schemes are all standalone modules that you can combine to create new models. New modules are simple to add, as new classes and functions.  \n",
    "\n",
    "tf.keras is TensorFlow's implementation of the Keras API specification. This is a high-level API to build and train models that includes first-class support for TensorFlow-specific functionality, such as eager execution, tf.data pipelines, and Estimators. tf.keras makes TensorFlow easier to use without sacrificing flexibility and performance.\n",
    "\n",
    "## Problem 10:\n",
    "\n",
    "Following the example below, modify it to generate the same network constructed in problem 8. The network must have the same number of hidden layers, neurons per layer and learning rate. Feel free to explore the tensorflow´s documentation where this example was taken. https://www.tensorflow.org/guide/keras#entire_model .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFY ME:\n",
    "    \n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(tf.VERSION)\n",
    "print(tf.keras.__version__)\n",
    "\n",
    "#Creates a Sequential model with Dense layers.\n",
    "model = tf.keras.Sequential()\n",
    "\n",
    "\n",
    "#Dense implements the operation:\n",
    "#        output = activation(dot(input, kernel) + bias)\n",
    "#Units are the dimensionality of the output space for the layer,\n",
    "#     which equals the number of hidden units\n",
    "#Activation and loss functions may be specified by strings or classes\n",
    "\n",
    "# Adds a densely-connected layer with 64 units to the model:\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "\n",
    "# Add another:\n",
    "model.add(layers.Dense(50, activation='relu'))\n",
    "\n",
    "# Add a softmax layer with 10 output units:\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "#The compile method configures the model’s learning process\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#The fit method does the training in batches\n",
    "model.fit(TRAIN_DATASET, TRAIN_LABELS, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The evaluate method calculates the losses and metrics\n",
    "#     for the trained mode\n",
    "loss_and_metrics = model.evaluate(TEST_DATASET, TEST_LABELS, batch_size=1280)\n",
    "\n",
    "#The predict method applies the trained model to inputs\n",
    "#     to generate outputs\n",
    "classes = model.predict(TEST_DATASET, batch_size=128)\n",
    "\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 11: Finally, in here, you are asked to modify the network architecture to improve the classification results.\n",
    "\n",
    "\n",
    "Your network can have any number of layers, you can use any loss function, and quality metric. __HOWEVER__ your resulting model must not exceed a total of 20,000 _learnable parameters_ -- This is the total number of weights to optimize in the sequential model. If you are not sure what we are talking about, here's a nice explanation.\n",
    "\n",
    "https://towardsdatascience.com/counting-no-of-parameters-in-deep-learning-models-by-hand-8f1716241889\n",
    "\n",
    "* Show the performance of your network using ONLY the training set. \n",
    "* Compute and explain the number fo parameters of your network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of hand-craft feature based SVM, and Random Forest, and MLP \n",
    "\n",
    "Regarding the neural networks with several layers with large number of neurons, an increase in the layers/neurons will increase the capacity of the network (not accuracy). \n",
    "\n",
    "To generalize these large networks a significantly large number of training samples are required (atleast 10 times the number of parameters in a network) otherwise it will simply over fit even with regularization constraints. However, for the problem with quite small samples, SVM (kernel), Logistic Regression, or Random Forest classifiers, etc. may gives better performance. \n",
    "\n",
    "On the other hand, when there are large number of training samples, the deep learning based model can reach much better performance. Here, we aim to train three models -- SVM, Random Forest and MLP -- using a varying number of training samples. For a fair comparision, we only use image raw features, but other hand-crafted features such as HOG can be used to train SVM model.\n",
    "\n",
    "\n",
    "For the random forest and SVM you should use the implementations included in sklearn with default parameters. If you wish to modify this paramters, be sure to use only the test dataset to perform the hyperparameter tuning. \n",
    "\n",
    "`from sklearn.svm import LinearSVC`\n",
    "`from sklearn.ensemble import RandomForestClassifier`\n",
    "\n",
    "## Problem 12: Write below your implementation of both classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here.\n",
    "\n",
    "def train_and_validate(NUM_SAMPLES):\n",
    "\n",
    "    \n",
    "    \n",
    "#Example\n",
    "TRAINING_SIZES = [1000, 2000]\n",
    "\n",
    "for size in TRAINING_SIZES:\n",
    "    score = train_and_validate(size)\n",
    "    print(\"Samples:\", size, \"---> Score:\", score)\n",
    "    \n",
    "# Samples: 1000 ---> Score: 0.7900200000000001\n",
    "# Samples: 2000 ---> Score: 0.78766\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 13\n",
    "\n",
    "Now, in here we want to train and compare a SVM model and a Random Foest model, against the MLP model that you created. You should use a varying number of training samples including 200, 500, 1000 ,5000, 8000 and 10000 images. \n",
    "\n",
    "At the end, you have to compute three accuracies (MLP accuracy, Random Forest acc. and the SVM acc.) vs number of training samples in one SINGLE plot, to have a better understanding of our models performance. With a increase in the number of training samples, the performance gap between two models will be more visible. \n",
    "\n",
    "Importantly, be sure to use now the __validation__ dataset to benchmark your classifiers and plot for, for each classifier, the obtained accuracy on the training set and in the validation set.  \n",
    "\n",
    "Why is this important? \n",
    "\n",
    "Show the obtained graph and comment on the performance obtained. \n",
    "\n",
    "The plot should contained __6 different \"lines\"__: 2 lines for each calissifier -- one for the validation accuracy, and one for the training accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_SIZES = [200, 500, 1000, 5000, 8000, 10000]\n",
    "\n",
    "#Code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

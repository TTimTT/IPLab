{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3 : Object Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import tarfile\n",
    "import time as _time\n",
    "from utils import *\n",
    "%matplotlib inline\n",
    "\n",
    "#Import all the extra you may need."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Video Processing and Detection. \n",
    "\n",
    "In this first part of chapter 3, you will be asked to construct a series of functions tools to handle video processing and basic detection. You will use the same HOG+SVM classifier from Chapter 2 and adapt it to handle video input.\n",
    "\n",
    "In contrast to the previous assignments, in this assignment, we will not provide a guided skeleton of the function. You will use any high level (already implemented) function from OpenCV to complete the exercise specifications and outputs. In addition, you will be asked to search in the documentation (or your favourite reference) the specific usage of the functions. \n",
    "\n",
    "__Section Objectives:__\n",
    "\n",
    "* Construct a Video processing pipeline.\n",
    "* Use OpenCV functions on the video frames for human detection.\n",
    "* Analyse and compare tracking methodes proposed by OpenCV\n",
    "\n",
    "__Data__:\n",
    "\n",
    "The chapter data included inside ``../data/videos`` folder contains three sample videos to test your functions.\n",
    "\n",
    "\n",
    "### 1.1 Reading/Writing video\n",
    "\n",
    "A video file can be abstracted as a bunch of images of the same dimensions in order (*i.e. collection*). Our first task is to complete the function `transform_video_file(...)` which will operate on every frames of a given video file. The following steps are required:\n",
    "\n",
    "- Open video file\n",
    "- Iterate overall frames or a subset of them (based on `n_frame` parameter)\n",
    "- Apply the processing function. The function's signature needs to be `function(np.array, Any) -> Any`\n",
    "\n",
    "To read videos you can use the OpenCV build-in functions, namely `cv::VideoCapture` class. More informations is available in the [docs](https://docs.opencv.org/3.4.4/d8/dfe/classcv_1_1VideoCapture.html#a473055e77dd7faa4d26d686226b292c1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_video_file(file_path, function, params=None, n_frame=-1):\n",
    "    \"\"\"\n",
    "    Given the path of a video file (file_path) the function reads every frame of the input video and applies a given\n",
    "    transformation (function) using the parameters (params)\n",
    "\n",
    "    :input_image:       Input video file path \n",
    "    :function:          Function be applied to each frame of the image. Signature `function(np.array, Any) -> Any`\n",
    "    :params:            Any parameter needed for the function above.\n",
    "    :n_frame:           Maxiumum number of frame to read. Default `-1`, read all the content\n",
    "    :return:            output_handler this can be anything you may need to save your results.\n",
    "    \"\"\"\n",
    "    output_handler = []\n",
    "    # Open video\n",
    "    cap = cv.VideoCapture(file_path)\n",
    "    if cap.isOpened():\n",
    "        \n",
    "        # Init\n",
    "        n = n_frame\n",
    "        while(n != 0):\n",
    "\n",
    "            # Capture frame-by-frame\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            # Our operations on the frame come here\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            print(n)\n",
    "\n",
    "            # Ending condition\n",
    "            n -= 1\n",
    "            if(not ret):\n",
    "                break\n",
    "\n",
    "        # When everything done, release the capture\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "                        \n",
    "    else:\n",
    "        raise ValueError('Can not open file: {}'.format(file_path))\n",
    "    # Close reader\n",
    "    cap.release()\n",
    "    # Return custom structure\n",
    "    return output_handler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Example:\n",
    "\n",
    "The example below shows how the `transform_video_file` can be used. It will open the video named `speaker.avi` and process only the first **2** frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of usage:\n",
    "def dummy_function(image, params):\n",
    "    if('dummy_function' in params[0]):\n",
    "        print('Func: \"{}\", Message \"{}\", File: \"{}\"'.format(params[0], params[1], params[2]))\n",
    "        print('Image dims: {}'.format(image.shape))\n",
    "    return 'I did it'\n",
    "# Define parameters\n",
    "file_name = '../data/videos/speaker.avi'\n",
    "extra_params = ['dummy_function', 'Hello from the video file', file_name];\n",
    "\n",
    "# Process single frame\n",
    "transform_video_file(file_path=file_name, function=dummy_function, params=extra_params, n_frame=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Testing your function:\n",
    "\n",
    "To test your function you need to process the video: ``../data/videos/video_gym.avi``. At each frame, you will count the number of __blue__ pixel. At the end you should return two outputs:\n",
    "\n",
    "* The __frame number__ with the maximum number of blue pixels\n",
    "* The __image__ frame with more blue pixels in greyscale, except for the blue pixels.\n",
    "\n",
    "Display the image and the frame number. \n",
    "\n",
    "__Hint:__ Consider a blue pixel as any pixel in the range of the blue color in HSV: \n",
    "``([90,120,120]) , [130,255,255])``. \n",
    "\n",
    "__Hint 2__: Check the first assignment if you have no idea what am I talking about.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_fn(image, params):\n",
    "    \"\"\"\n",
    "    Count the number of blue pixels in a given image. This function also extracts the region where the pixels are blue\n",
    "    in form of a mask.\n",
    "    \n",
    "    :param image:  Frame to analyse\n",
    "    :param params: Extra parameters that my be required by the functions\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "\n",
    "    # Done\n",
    "    return (None, None) # Magic structure with the number of blue pixels + mask\n",
    "\n",
    "\n",
    "# Video file\n",
    "file_path =  os.path.join('..','data', 'videos', 'video_gym.avi')\n",
    "outputs = transform_video_file(file_path, processing_fn);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At each 30 seconds of the video, on its repectively frame (i.e. `frame_idx % 30 == 0`), show the following results\n",
    "\n",
    "- Extracted mask\n",
    "- Number of blue pixels counted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: The return of the HOG.\n",
    "\n",
    "Do you remember HOG? No? Well, it's okay, since all you need is to remember how to use it, here is a small reminder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the HOG descriptor/person detector\n",
    "hog = cv.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "\n",
    "# load base image (check that we are not scaling, normalizing or changing the channels)\n",
    "img = cv.imread(os.path.join('..','data', 'person_454.bmp'))\n",
    "print('Image has dimensions: {}'.format(img.shape))\n",
    "\n",
    "# The HOG detector returns an array with the Regions of maximum likehood to contain a human-shaped-form\n",
    "rects, weights = hog.detectMultiScale(img , winStride=(4, 4), padding=(8, 8), scale=1.05)\n",
    "\n",
    "# draw the original bounding boxes\n",
    "persons = 0;\n",
    "for k, (x, y, w, h) in enumerate(rects):\n",
    "    cv2.rectangle(img , (x, y), (x + w - 1, y + h -1), (0, 255, k * 256), 2)\n",
    "    \n",
    "display_image(img);\n",
    "plt.title('Detection results')\n",
    "plt.xticks([])\n",
    "plt.yticks([]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, you will glue together your brand new image processing function and the HOG descriptor from OpenCV above.\n",
    "\n",
    "### Testing your function:\n",
    "\n",
    "To test your function you need to process the video: ``../data/videos/video_skater.avi``. At each frame, you will use HOG to detect any person in the frame. At the end of the function you should return: \n",
    "\n",
    "* The image __frame__ corresponding to __the seconds 1, 2, ... , 10__ of the video with the rectangle showing the \"detected area\". \n",
    "\n",
    "__Hint __: Check the second assignment if you have no idea what am I talking about.\n",
    "\n",
    "__Be sure to display all the 10 frames for grading__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_fn(detector, image, params):\n",
    "    \"\"\"\n",
    "    Run pedestrian detector on a given image.\n",
    "    \n",
    "    :param detector: HOG Detector instance\n",
    "    :param image: Image to process\n",
    "    :param params: Extra parameters needed by the function\n",
    "    :return: Detected pedestrian bounding boxes\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "\n",
    "    return None, None\n",
    "    \n",
    "# Video file\n",
    "file_path =  os.path.join('..','data', 'videos', 'video_skater.avi')\n",
    "outputs = transform_video_file(file_path, lambda im, p: detection_fn(hog, im, p), n_frame=350);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# YOUR CODE HERE\n",
    "img, (x, y, w, h) = outputs[0]\n",
    "im = cv2.rectangle(img.copy() , (x, y), (x + w - 1, y + h -1), (255, 0, 0), 2, lineType=cv.LINE_AA)\n",
    "display_image(im)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Face Tracking\n",
    "\n",
    "Up to now, the frames of the video were considered as single image having no relationship between them. However, this is not the best approach: the correlation between two consecutive frames is large, since the image will displace or change in the next frame with respect to the previous one only by few pixels. We can rely on this to build a smarter detector. \n",
    "\n",
    "The case study for the next sections will be face tracking. The task is to provide a bounding box where the face is located. A number of methods will be analysed and benchmarked against each other. The baseline will be established with a standard face detector based on [Viola, Jones](https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf) work (*i.e. no tracking*). Then, you will compare it against the tracking algorithms.\n",
    "\n",
    "The methods to use and compare are the following:\n",
    "\n",
    "- Frame-wise face detection (Baseline)\n",
    "- Tracking: 'MIL', 'KCF', 'TLD', 'MedianFlow', 'Mosse'\n",
    "\n",
    "For comparison, you will be asked  to implemented the following metrics:\n",
    "\n",
    "- Euclidean distance between center's bounding box\n",
    "- Intersection over Union\n",
    "- Computation time\n",
    "\n",
    "Your first task is to implement the preprocessing function that will be applied on every frame before actually doing the tracking. You can apply any transformation you want to each frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all images\n",
    "def preprocessing_fn(image, params):\n",
    "    \"\"\"\n",
    "    Apply preprocessing on a given image.\n",
    "    \n",
    "    :param image: Image to preprocess\n",
    "    :param params: Extra parameters\n",
    "    :return: Preprocessed image\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return None\n",
    "\n",
    "file_path =  os.path.join('..','data', 'videos', 'speaker.avi')\n",
    "images = transform_video_file(file_path, preprocessing_fn, n_frame=350)\n",
    "print('Video contains a total of {} frames'.format(len(images)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Face Detection\n",
    "\n",
    "The detection baseline is established using an instance of `cv::CascadeClassifier` which implements the approach proposed by Viola & Jones for object detection. \n",
    "\n",
    "Your task is to complete the implementation of the `detection_fn` which, given an instance of a classifier and an image, return the bounding box where the face is located. More information about the detector can be found in the [doc](https://docs.opencv.org/3.4.4/d1/de5/classcv_1_1CascadeClassifier.html#ab3e572643114c43b21074df48c565a27).\n",
    "\n",
    "Be sure to implement a solution to deal with the situation where multiple boxes are returned by the classifier. The function must return **ONLY** one bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detection_fn(detector, image):\n",
    "    \"\"\"\n",
    "    Run face detection on a given `image` with an instance of CascadeClassifier\n",
    "    \n",
    "    :param detector: CascadeClassifier instance\n",
    "    :param image: Frame on which to run detection\n",
    "    :return: bounding box if any or None\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the following cell will test your `detection_fn` and display the outcome for the first four frames of the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load face detector\n",
    "fpath = os.path.join('..', 'data', 'haarcascade_frontalface_alt2.xml')\n",
    "fdet = cv.CascadeClassifier(fpath)\n",
    "assert fdet.empty() is not True\n",
    "\n",
    "# Detect \n",
    "detection_out= []\n",
    "for img in images[:4]:\n",
    "    bbox = detection_fn(fdet, img)\n",
    "    detection_out.append(bbox)\n",
    "    \n",
    "# Display\n",
    "fig, ax = plt.subplots(2, 2, figsize=(17, 9))\n",
    "for k, box in enumerate(detection_out):\n",
    "    # Draw rectangle\n",
    "    x, y, w, h = box\n",
    "    im = cv.rectangle(images[k].copy(), (x, y), (x + w -1, y + h -1), (255, 0, 0), 3, lineType=cv.LINE_AA)\n",
    "    # Display\n",
    "    r = k // 2\n",
    "    c = k % 2\n",
    "    display_image(im, axes=ax[r][c])\n",
    "    ax[r][c].set_title('Detection at frame {}'.format(k))\n",
    "    ax[r][c].set_xticks([])\n",
    "    ax[r][c].set_yticks([]);    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Face Tracking\n",
    "\n",
    "Similar to what you have done before, you will have to complete the `tracking_fn` that will perform the tracking step. Given one instance of `cv::Tracker` [(doc)](https://docs.opencv.org/3.4.4/d0/d0a/classcv_1_1Tracker.html) and a  input image, the function will return the bounding box where the object (*i.e. face*) is located.\n",
    "\n",
    "Again, be sure to **ONLY** one bounding box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tracking_fn(tracker, image):\n",
    "    \"\"\"\n",
    "    Perform tracking on a given image.\n",
    "    \n",
    "    :param tracker: Tracker instance\n",
    "    :param image: Image on which to run tracker\n",
    "    :return: Bounding box if any\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will test your `tracking_fn` in the same way as what we did earlier b traking the first four frames of the video and displaying the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tracker\n",
    "tracker = create_face_tracker(name='KCF')\n",
    "tracker.init(images[0], (551, 121, 153, 153))\n",
    "\n",
    "# Detect \n",
    "tracking_out=[]\n",
    "for img in images[:4]:\n",
    "    bbox = tracking_fn(tracker, img)\n",
    "    tracking_out.append(bbox)\n",
    "           \n",
    "# Display\n",
    "fig, ax = plt.subplots(2, 2, figsize=(17, 9))\n",
    "for k, box in enumerate(tracking_out):\n",
    "    # Draw rectangle\n",
    "    x, y, w, h = box\n",
    "    x = int(x); y=int(y); w=int(w); h=int(h)\n",
    "    im = cv.rectangle(images[k].copy(), (x, y), (x + w -1, y + h -1), (0, 255, 0), 3, lineType=cv.LINE_AA)\n",
    "    # Display\n",
    "    r = k // 2\n",
    "    c = k % 2\n",
    "    display_image(im, axes=ax[r][c])\n",
    "    ax[r][c].set_title('Tracking at frame {}'.format(k))\n",
    "    ax[r][c].set_xticks([])\n",
    "    ax[r][c].set_yticks([]);    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Comparison\n",
    "\n",
    "#### 2.4.1 Metric\n",
    "\n",
    "The performance of a given tracker will be assessed with the following metrics:\n",
    "\n",
    "- Intersection over Union\n",
    "- Distance between center's bounding boxes\n",
    "- Execution time\n",
    "\n",
    "##### 2.4.1.1 Intersection over Union\n",
    "\n",
    "Your task is to complete the implementation of the `iou_metric` function. The function will return the *Intersection over Union* given two bounding boxes, namely `box_a` and `box_b`.\n",
    "\n",
    "You can find the metric as the Jaccard index https://en.wikipedia.org/wiki/Jaccard_index. Feel free to modify the implementation below to adapt it to your function output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou_metric(box_a, box_b):\n",
    "    \"\"\"\n",
    "    Compute the Intersection over Union (IoU) between two bounding boxes (x, y, w, h)\n",
    "    \n",
    "    :param box_a: First bounding box\n",
    "    :param box_b: Second bounding box\n",
    "    :return: Intersction over Union\n",
    "    \"\"\"\n",
    "    iou = 0.0\n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # return the intersection over union value\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage + sanity check\n",
    "IoU = iou_metric(box_a=[39, 63, 164, 49], box_b=[40, 63, 165, 49])\n",
    "print('Intersection over Union: {:.3f}'.format(IoU))\n",
    "assert round(IoU, 3) == 0.982, 'Somehting went wrong at your implementation of the `iou_metric` function'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4.1.2 Center's distance\n",
    "\n",
    "Complete the `center_metric` function that compute the euclidean distance between two bounding box centers, define as:\n",
    "\n",
    "$$\n",
    "d_i = \\left|\\left| c_i^A - c_i^B \\right|\\right|\n",
    "$$\n",
    "\n",
    "where `A` and `B` are the two bounding boxes (*i.e. detection + ground truth*)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_metric(box_a, box_b):\n",
    "    \"\"\"\n",
    "    Compute the Euclidean distance between two given bounding boxes\n",
    "    \n",
    "    :param box_a: First bounding box\n",
    "    :param box_b: Second bounding box\n",
    "    :return: Distance\n",
    "    \"\"\"\n",
    "    d = 0.0\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return d\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage + sanity check\n",
    "dist = center_metric(box_a=[39, 63, 164, 49], box_b=[40, 63, 165, 49])\n",
    "print('Distance: {:.3f}'.format(dist))\n",
    "assert round(dist, 3) == 1.500, 'Somehting went wrong at your implementation of the `center_metric` function'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4.1.3 Execution time\n",
    "\n",
    "The function `time_metric` will measure the execution time of a given function defined in `processing_fn`. Its signature must be `processing_fn() -> Any`.\n",
    "If the `processing_fn` return some values, they will be passed through the `time_metric` function. Therefore the return value is a tuple containing the execution time in first position and anything returned by `processing_fn` in the second position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_metric(processing_fn):\n",
    "    \"\"\"\n",
    "    Measure the execution time of a given function (i.e. lambda function)\n",
    "    \n",
    "    :param processing_fn:  Lambda function to be timed\n",
    "    :return: Execution time in milli-seconds\n",
    "    \"\"\"\n",
    "    s0 = cv.getTickCount()\n",
    "    retval = processing_fn()\n",
    "    s1 = cv.getTickCount()\n",
    "    return (1000.0 * (s1 - s0)) / cv.getTickFrequency(), retval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `time_metric` function can be used as follow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using user define function with proper prototype\n",
    "def wasting_time_func():\n",
    "    _time.sleep(0.75)\n",
    "    return 'Done'\n",
    "    \n",
    "# Call metric\n",
    "dt, ret = time_metric(wasting_time_func)\n",
    "print('User-defined function took: {:.3f} ms, with return value {}'.format(dt, ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using lambda function \n",
    "dt, ret = time_metric(lambda: _time.sleep(0.5))\n",
    "print('Lambda function took: {:.3f} ms'.format(dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.2 Load Ground Truth\n",
    "\n",
    "The true bounding boxes are stored into the `data/videos/speaker_gt.txt` text file. Each line contains the true bounding box for the corresponding frame.\n",
    "\n",
    "Your task is to complete the `load_ground_truth` that parses the file and return a list of boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ground_truth(filename):\n",
    "    \"\"\"\n",
    "    Load all detection stored into a given file\n",
    "    \n",
    "    :param filename: Path to the text file storing the ground truth\n",
    "    :return: List of bounding boxes\n",
    "    \"\"\"\n",
    "    bboxes = []\n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load every face bounding boxes into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth\n",
    "fname = os.path.join('..', 'data', 'videos', 'speaker_gt.txt')\n",
    "gt_bbox = load_ground_truth(filename=fname)\n",
    "print('There is a total of {} bounding boxes'.format(len(gt_bbox)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 2.4.3 Experimental setup\n",
    "\n",
    "In the `run_experiment` function, given a list of pair `{Image, Ground truth}`, it will perform the tracking/detection on all images and run the various *metrics* that have been implemented earlier. \n",
    "\n",
    "The results for a **single** frame will be stored into a dictionary with the following entries:\n",
    "\n",
    "- `bbox` will contain the detected region\n",
    "- `metrics` will contain all the metrics computed during the experiment. The metrics are stored in a `tuple` ordered as : `IoU, Distance, Time`\n",
    "\n",
    "\n",
    "The next cells run the experiment with the `detection_fn` in order to establish the baseline. Later, on the tracking results, all the metrics will be compared against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(images, true_bbox, tracking_fn):\n",
    "    \"\"\"\n",
    "    Run a given detection/tracking function on a list of images.\n",
    "    \n",
    "    :param images: List of consecutive images to processed (already preprocessed)\n",
    "    :param bboxes: List of true bounding boxes (Ground truth)\n",
    "    :param tracking_fn: Function running detection/tracking for ONE frame\n",
    "    :return: List of dictionnaries for each frames holding detected bounding box if any and the various metrics\n",
    "    \"\"\"\n",
    "    metrics = []\n",
    "    for img, gt_bbox in zip(images, true_bbox):\n",
    "        # Run metrics + tracking\n",
    "        dt, bbox = time_metric(lambda: tracking_fn(img))        \n",
    "        iou = iou_metric(bbox, gt_bbox)\n",
    "        dist = center_metric(bbox, gt_bbox)\n",
    "        # Accumulate results\n",
    "        metrics.append({'bbox': bbox, 'metric': (iou, dist, dt)})\n",
    "    # Done\n",
    "    return metrics\n",
    "\n",
    "# Run detection, no tracking\n",
    "detection_exp = run_experiment(images=images, true_bbox=gt_bbox, tracking_fn=lambda x: detection_fn(fdet, x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to run the same experiment (*i.e. `run_experiment`*) for all the selected trackers. To instantiate the tracker using its name, you have to use the function `create_face_tracker(str) -> cv::Tracker`.\n",
    "\n",
    "For the initialisation of the tracker, you can use the region: `(551, 121, 175, 175)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of tracker to use\n",
    "trackers_name = ['MIL', 'KCF', 'TLD', 'MedianFlow', 'Mosse']\n",
    "# Run experiments for each tracker\n",
    "trackers_exp = {}\n",
    "for name in trackers_name:\n",
    "    \n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.4 Results\n",
    "\n",
    "In this section we ask you to display and discuss about the outcome of the tracking experiments. Compare every tracker with the baseline established earlier and discuss what you observed.\n",
    "\n",
    "Report the following quantities:\n",
    "\n",
    "- The `min`, `max`, `mean`, `std` of each metrics for every tracker and the baseline, comment on the values\n",
    "- Display the detection/tracking of the `best` and the `worst` IoU for each algorithm\n",
    "- Discuss the performance of each tracker\n",
    "- Discuss the pros and cons of each metrics\n",
    "- Investigate each tracker and explain briefly what is the methods used under the hood.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# YOUR CODE HERE\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
